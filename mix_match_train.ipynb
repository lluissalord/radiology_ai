{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.11 64-bit ('radiology_ai': conda)",
   "display_name": "Python 3.6.11 64-bit ('radiology_ai': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73c679d2cc001810287e7be6e0757dc766f509031127652202e04048c8fced99"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    PATH_DRIVER = '/content/gdrive/My Drive/'\n",
    "    DATA_FOLDER = 'Dataset/'\n",
    "else:\n",
    "    PATH_DRIVER = ''\n",
    "    DATA_FOLDER = 'C:/Users/Lluis/Desktop/Machine Learning/radiology_ai/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import *\n",
    "from fastai.data.transforms import *\n",
    "from fastai.vision import models\n",
    "from fastai.vision.augment import *\n",
    "from fastai.vision.core import PILImageBW\n",
    "from fastai.vision.data import *\n",
    "from fastai.vision.learner import create_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixmatch.losses import MixMatchLoss\n",
    "from mixmatch.callback import MixMatchCallback\n",
    "from mixmatch.utils import TestColSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREFIX = os.path.join(PATH_DRIVER, DATA_FOLDER, '')\n",
    "raw_folder = PATH_PREFIX + 'DICOMS'\n",
    "organize_folder = PATH_PREFIX + 'pending_classification'\n",
    "preprocess_folder = PATH_PREFIX + 'preprocess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "LR = 0.002\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "RESIZE = 512\n",
    "RANDOM_RESIZE_CROP = 256\n",
    "\n",
    "T = 0.5\n",
    "LAMBDA_U = 75\n",
    "ALPHA = 0.75\n",
    "# EMA_DECAY = 0.999\n",
    "\n",
    "MODEL = models.resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "label_transform = [\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "    Flip(),\n",
    "    Normalize()\n",
    "]\n",
    "\n",
    "class Multiply_255(Transform):\n",
    "    def encodes(self, o): return o * 255\n",
    "\n",
    "unlabel_transform = [\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "    Flip(),\n",
    "    Rotate(180, p=1),\n",
    "    Multiply_255(),\n",
    "    # Normalize()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "\n",
    "cbs = None\n",
    "cbs = [TensorBoardCallback()]\n",
    "# cbs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_excel(os.path.join(PATH_PREFIX, 'all.xlsx'), dtype={'ID':'string','Target':'string'})\n",
    "\n",
    "unlabel_df = df[['ID','Target']][df['Target'].isnull()].reset_index(drop=True)\n",
    "label_df = df[['ID','Target']][df['Target'].notnull()].reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(label_df, test_size=0.15, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, shuffle=True)\n",
    "\n",
    "label_df.loc[train_df.index, 'Dataset'] = 'train'\n",
    "label_df.loc[val_df.index, 'Dataset'] = 'valid'\n",
    "label_df.loc[test_df.index, 'Dataset'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> Preparing label dataloaders\n",
      "==> Preparing unlabel dataloaders\n",
      "==> Preparing MixMatch callback\n"
     ]
    }
   ],
   "source": [
    "# DataLoaders\n",
    "print(f'==> Preparing label dataloaders')\n",
    "\n",
    "label_dl = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), MultiCategoryBlock),\n",
    "    get_x=ColReader('ID', pref=preprocess_folder+'/', suff='.png'), \n",
    "    get_y=ColReader('Target'),\n",
    "    splitter=TestColSplitter(col='Dataset'),\n",
    "    item_tfms=Resize(RESIZE),\n",
    "    batch_tfms=label_transform,\n",
    ").dataloaders(label_df, bs=BATCH_SIZE, num_workers=0, shuffle_train=True, drop_last=True)\n",
    "\n",
    "print(f'==> Preparing unlabel dataloaders')\n",
    "\n",
    "ds_params = {\n",
    "    'blocks': (ImageBlock(cls=PILImageBW)),\n",
    "    'get_x': ColReader('ID', pref=preprocess_folder+'/', suff='.png'),\n",
    "    'splitter': RandomSplitter(0),\n",
    "    'item_tfms': Resize(RESIZE)\n",
    "}\n",
    "dls_params = {\n",
    "    'source': unlabel_df,\n",
    "    'bs': BATCH_SIZE,\n",
    "    'num_workers': 0,\n",
    "    'shuffle_train': True,\n",
    "    'drop_last': True\n",
    "}\n",
    "\n",
    "unlabel_dl = DataBlock(\n",
    "    **ds_params\n",
    ").dataloaders(**dls_params)\n",
    "\n",
    "transform_dl = DataBlock(\n",
    "    **ds_params,\n",
    "    batch_tfms = unlabel_transform\n",
    ").dataloaders(**dls_params)\n",
    "\n",
    "print(f'==> Preparing MixMatch callback')\n",
    "\n",
    "if cbs is None:\n",
    "    cbs = [MixUp(ALPHA)]\n",
    "else:\n",
    "    cbs.append(MixUp(ALPHA))\n",
    "\n",
    "cbs.append(MixMatchCallback(unlabel_dl, transform_dl, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> creating model\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"==> creating model\")\n",
    "\n",
    "def create_model(model_arq, n_out, pretrained=True, n_in=1, ema=False):\n",
    "    model = create_cnn_model(model_arq, n_out=n_out, cut=None, pretrained=pretrained, n_in=n_in)\n",
    "    model = model.cuda()\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model\n",
    "\n",
    "classes = label_df['Target'].unique()\n",
    "n_out = len(classes)\n",
    "\n",
    "model = create_model(MODEL, n_out, pretrained=True, n_in=1)\n",
    "# ema_model = create_model(MODEL, n_out, pretrained=True, n_in=1, ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> defining loss\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "print(\"==> defining loss\")\n",
    "\n",
    "class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['Target'])\n",
    "class_weight = torch.as_tensor(class_weight).float()\n",
    "if torch.cuda.is_available():\n",
    "    class_weight = class_weight.cuda()\n",
    "\n",
    "train_criterion = MixMatchLoss(unlabel_dl=unlabel_dl, model=model, n_out=n_out, bs=BATCH_SIZE, lambda_u=LAMBDA_U, weight=class_weight)\n",
    "criterion = train_criterion.Lx_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==> defining learner\n",
      "epoch     train_loss  valid_loss  f1_score  precision_score  recall_score  time    \n",
      "0         20.849142   0.008246    0.500000  0.500000         0.500000      00:32     \n"
     ]
    }
   ],
   "source": [
    "# Learner\n",
    "print(\"==> defining learner\")\n",
    "\n",
    "Lx_metric = AvgMetric(func=criterion)\n",
    "Lu_metric = AvgMetric(func=train_criterion.Lu_criterion)\n",
    "\n",
    "f1_score = F1ScoreMulti(average='macro')\n",
    "precision = PrecisionMulti(average='macro')\n",
    "recall = RecallMulti(average='macro')\n",
    "fastai_metrics = [\n",
    "    # Lx_metric, Lu_metric, \n",
    "    f1_score, precision, recall\n",
    "]\n",
    "\n",
    "# cbs.append(Recorder(train_metrics=True))\n",
    "\n",
    "learn = Learner(label_dl, model, loss_func=train_criterion, lr=LR, metrics=fastai_metrics, cbs=cbs)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}