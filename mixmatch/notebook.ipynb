{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.11 64-bit ('radiology_ai_36': conda)",
   "display_name": "Python 3.6.11 64-bit ('radiology_ai_36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ab27f263fa3fe7a1ad6d9a75f4efe68779144ca0aa63fb9b5bf3805584195823"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dataset.cifar10 as dataset\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "from fastai.vision.learner import create_cnn_model\n",
    "from fastai.vision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization options\n",
    "epochs=3\n",
    "start_epoch=0\n",
    "batch_size=32\n",
    "lr=0.002\n",
    "# Checkpoints\n",
    "resume=''\n",
    "# Miscs\n",
    "manualSeed=0\n",
    "#Device options\n",
    "gpu=0\n",
    "#Method options\n",
    "train_iteration=32\n",
    "out='result'\n",
    "alpha=0.75\n",
    "lambda_u=75\n",
    "T=0.5\n",
    "ema_decay=0.999\n",
    "\n",
    "n_out = 2\n",
    "model_arq = models.resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "np.random.seed(manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(labeled_trainloader, unlabeled_trainloader, model, n_out, optimizer, ema_optimizer, criterion, epoch, use_cuda):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    ws = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Training', max=train_iteration)\n",
    "    labeled_train_iter = iter(labeled_trainloader)\n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    # for batch_idx in tqdm(range(train_iteration), desc='Train iteration'):\n",
    "    for batch_idx in range(train_iteration):\n",
    "        try:\n",
    "            inputs_x, targets_x = labeled_train_iter.next()\n",
    "        except:\n",
    "            labeled_train_iter = iter(labeled_trainloader)\n",
    "            inputs_x, targets_x = labeled_train_iter.next()\n",
    "\n",
    "        try:\n",
    "            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n",
    "        except:\n",
    "            unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        batch_size = inputs_x.size(0)\n",
    "\n",
    "        # Transform label to one-hot\n",
    "        targets_x = torch.zeros(batch_size, n_out).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "            inputs_u = inputs_u.cuda()\n",
    "            inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute guessed labels of unlabel samples\n",
    "            outputs_u = model(inputs_u)\n",
    "            outputs_u2 = model(inputs_u2)\n",
    "            p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "            pt = p**(1/T)\n",
    "            targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "            targets_u = targets_u.detach()\n",
    "\n",
    "        # mixup\n",
    "        all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
    "        all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "\n",
    "        l = np.random.beta(alpha, alpha)\n",
    "\n",
    "        l = max(l, 1-l)\n",
    "\n",
    "        idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "        input_a, input_b = all_inputs, all_inputs[idx]\n",
    "        target_a, target_b = all_targets, all_targets[idx]\n",
    "\n",
    "        mixed_input = l * input_a + (1 - l) * input_b\n",
    "        mixed_target = l * target_a + (1 - l) * target_b\n",
    "\n",
    "        # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
    "        mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "        mixed_input = interleave(mixed_input, batch_size)\n",
    "\n",
    "        logits = [model(mixed_input[0])]\n",
    "        for input in mixed_input[1:]:\n",
    "            logits.append(model(input))\n",
    "\n",
    "        # put interleaved samples back\n",
    "        logits = interleave(logits, batch_size)\n",
    "        logits_x = logits[0]\n",
    "        logits_u = torch.cat(logits[1:], dim=0)\n",
    "\n",
    "        Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/train_iteration)\n",
    "\n",
    "        loss = Lx + w * Lu\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), inputs_x.size(0))\n",
    "        losses_x.update(Lx.item(), inputs_x.size(0))\n",
    "        losses_u.update(Lu.item(), inputs_x.size(0))\n",
    "        ws.update(w, inputs_x.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | W: {w:.4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=train_iteration,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    loss_x=losses_x.avg,\n",
    "                    loss_u=losses_u.avg,\n",
    "                    w=ws.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "\n",
    "    return (losses.avg, losses_x.avg, losses_u.avg,)\n",
    "\n",
    "def validate(valloader, model, criterion, epoch, use_cuda, mode):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar(f'{mode}', max=len(valloader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs, targets)\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            # top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(valloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        # top5=top5.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "        bar.finish()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint=out, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def linear_rampup(current, rampup_length=epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "class SemiLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "\n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        Lu = torch.mean((probs_u - targets_u)**2)\n",
    "\n",
    "        return Lx, Lu, lambda_u * linear_rampup(epoch)\n",
    "\n",
    "class WeightEMA(object):\n",
    "    def __init__(self, model, ema_model, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.alpha = alpha\n",
    "        self.params = list(model.state_dict().values())\n",
    "        self.ema_params = list(ema_model.state_dict().values())\n",
    "        self.wd = 0.02 * lr\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if ema_param.dtype==torch.float32:\n",
    "                ema_param.mul_(self.alpha)\n",
    "                ema_param.add_(param * one_minus_alpha)\n",
    "                # customized weight decay\n",
    "                param.mul_(1 - self.wd)\n",
    "\n",
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def normalise(x, mean, std):\n",
    "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
    "    x -= mean\n",
    "    x *= 1.0/std\n",
    "    return x\n",
    "\n",
    "def transpose(x, source='NHWC', target='NCHW'):\n",
    "    return x.transpose([source.index(d) for d in target]) \n",
    "\n",
    "class MixMatchDataset(Dataset):\n",
    "    \"\"\"MixMatch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, suff='', labeled=True, transform=None, size=512):\n",
    "        self.df = df\n",
    "        if labeled:\n",
    "            self.df.iloc[:, 1] = self.df.iloc[:, 1].astype('category').cat.codes\n",
    "        self.root_dir = root_dir\n",
    "        self.suff = suff\n",
    "        self.labeled = labeled\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "\n",
    "        self.data = []\n",
    "        for idx in tqdm(range(len(df)), desc='Init images: '):\n",
    "            img_name = os.path.join(self.root_dir,\n",
    "                                    self.df.iloc[idx, 0] + self.suff)\n",
    "            image = Image.open(img_name)\n",
    "            image = transforms.Resize(self.size)(image)\n",
    "            image = np.array(image)\n",
    "            self.data.append(image)\n",
    "        self.data = np.array(self.data)\n",
    "\n",
    "        self.data_mean = np.mean(self.data, axis=(0,1,2)) # or use ImageNet stats (dividing all by 255)\n",
    "        self.data_std = np.std(self.data, axis=(0,1,2)) # or use ImageNet stats (dividing all by 255)\n",
    "\n",
    "        self.data = normalise(self.data, self.data_mean, self.data_std)\n",
    "        if len(self.data.shape) > 3:\n",
    "            self.data = transpose(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Use this if not all images can fit in-memory when __init__\n",
    "        # img_name = os.path.join(self.root_dir,\n",
    "        #                         self.df.iloc[idx, 0] + self.suff)\n",
    "        # image = Image.open(img_name)\n",
    "        # image = normalise(image, self.data_mean, self.data_std)\n",
    "        # if len(image.shape) > 2:\n",
    "        #     image = transpose(, source='HWC', target='CHW')\n",
    "\n",
    "        image = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            if self.labeled:\n",
    "                image = self.transform(image)\n",
    "                target = torch.as_tensor(self.df.iloc[idx, 1], dtype=torch.long)\n",
    "            else:\n",
    "                image = (self.transform(image), self.transform(image))\n",
    "                target = -1\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Init images:   2%|▏         | 3/146 [00:00<00:05, 25.71it/s]==> Preparing dataset\n",
      "Init images: 100%|██████████| 146/146 [00:05<00:00, 27.44it/s]\n",
      "Init images: 100%|██████████| 443/443 [00:17<00:00, 25.47it/s]\n",
      "Init images: 100%|██████████| 26/26 [00:00<00:00, 27.73it/s]\n",
      "Init images: 100%|██████████| 31/31 [00:01<00:00, 25.17it/s]\n",
      "==> creating model\n",
      "    Total params: 11.70M\n",
      "\n",
      "Epoch: [1 | 3] LR: 0.002000\n",
      "\n",
      "Epoch: [2 | 3] LR: 0.002000\n",
      "\n",
      "Epoch: [3 | 3] LR: 0.002000\n",
      "Best acc:\n",
      "84.6153793334961\n",
      "Mean acc:\n",
      "72.04301071166992\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(out):\n",
    "    mkdir_p(out)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    PATH_DRIVER = '/content/gdrive/My Drive/'\n",
    "    DATA_FOLDER = 'Dataset/'\n",
    "else:\n",
    "    PATH_DRIVER = ''\n",
    "    DATA_FOLDER = 'C:/Users/Lluis/Desktop/Machine Learning/radiology_ai/data/'\n",
    "\n",
    "PATH_PREFIX = os.path.join(PATH_DRIVER, DATA_FOLDER, '')\n",
    "raw_folder = PATH_PREFIX + 'DICOMS'\n",
    "organize_folder = PATH_PREFIX + 'pending_classification'\n",
    "preprocess_folder = PATH_PREFIX + 'preprocess'\n",
    "\n",
    "df = pd.read_excel(os.path.join(PATH_PREFIX, 'all.xlsx'), dtype={'ID':'string','Target':'string'})\n",
    "label_df = df[['ID','Target']][df['Target'].notnull()]\n",
    "train_df, test_df = train_test_split(label_df, test_size=0.15, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, shuffle=True)\n",
    "\n",
    "# Data\n",
    "print(f'==> Preparing dataset')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.Resize(512),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    # transforms.Resize(512),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "labeled_trainloader = data.DataLoader(\n",
    "    MixMatchDataset(train_df, root_dir=preprocess_folder, suff='.png', labeled=True, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "unlabeled_trainloader = data.DataLoader(\n",
    "    MixMatchDataset(df[['ID','Target']][df['Target'].isnull()], root_dir=preprocess_folder, suff='.png', labeled=False, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = data.DataLoader(\n",
    "    MixMatchDataset(val_df, root_dir=preprocess_folder, suff='.png', labeled=True, transform=transform_val),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    MixMatchDataset(test_df, root_dir=preprocess_folder, suff='.png', labeled=True, transform=transform_val),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Model\n",
    "print(\"==> creating model\")\n",
    "\n",
    "def create_model(model_arq, n_out, pretrained=True, n_in=1, ema=False):\n",
    "    model = create_cnn_model(model_arq, n_out=n_out, cut=None, pretrained=pretrained, n_in=n_in)\n",
    "    model = model.cuda()\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model(model_arq, n_out, pretrained=True, n_in=1)\n",
    "ema_model = create_model(model_arq, n_out, pretrained=True, n_in=1, ema=True)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "train_criterion = SemiLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ema_optimizer= WeightEMA(model, ema_model, alpha=ema_decay)\n",
    "start_epoch = 0\n",
    "\n",
    "# Resume\n",
    "title = 'noisy-cifar-10'\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "    out = os.path.dirname(resume)\n",
    "    checkpoint = torch.load(resume)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    ema_model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    logger = Logger(os.path.join(out, 'log.txt'), title=title, resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(out, 'log.txt'), title=title)\n",
    "    logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U',  'Valid Loss', 'Valid Acc.', 'Test Loss', 'Test Acc.'])\n",
    "\n",
    "# writer = SummaryWriter(out)\n",
    "step = 0\n",
    "test_accs = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, lr))\n",
    "\n",
    "    train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, n_out, optimizer, ema_optimizer, train_criterion, epoch, use_cuda)\n",
    "    _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\n",
    "    val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode='Valid Stats')\n",
    "    test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats ')\n",
    "\n",
    "    step = train_iteration * (epoch + 1)\n",
    "\n",
    "    # writer.add_scalar('losses/train_loss', train_loss, step)\n",
    "    # writer.add_scalar('losses/valid_loss', val_loss, step)\n",
    "    # writer.add_scalar('losses/test_loss', test_loss, step)\n",
    "\n",
    "    # writer.add_scalar('accuracy/train_acc', train_acc, step)\n",
    "    # writer.add_scalar('accuracy/val_acc', val_acc, step)\n",
    "    # writer.add_scalar('accuracy/test_acc', test_acc, step)\n",
    "\n",
    "    # append logger file\n",
    "    logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n",
    "\n",
    "    # save model\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(val_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'ema_state_dict': ema_model.state_dict(),\n",
    "            'acc': val_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "    test_accs.append(test_acc)\n",
    "logger.close()\n",
    "# writer.close()\n",
    "\n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n",
    "print('Mean acc:')\n",
    "print(np.mean(test_accs[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}