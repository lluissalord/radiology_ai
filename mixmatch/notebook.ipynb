{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.11 64-bit ('radiology_ai_36': conda)",
   "display_name": "Python 3.6.11 64-bit ('radiology_ai_36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ab27f263fa3fe7a1ad6d9a75f4efe68779144ca0aa63fb9b5bf3805584195823"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as torch_transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import dataset.cifar10 as dataset\n",
    "from utils import AverageMeter, accuracy, mkdir_p\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.data.block import *\n",
    "from fastai.vision import models\n",
    "from fastai.vision.augment import *\n",
    "from fastai.vision.core import PILImageBW\n",
    "from fastai.vision.data import *\n",
    "from fastai.vision.learner import create_cnn_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization options\n",
    "epochs=20\n",
    "start_epoch=0\n",
    "batch_size=32\n",
    "lr=0.002\n",
    "# Checkpoints\n",
    "resume=''\n",
    "# Miscs\n",
    "manualSeed=0\n",
    "#Device options\n",
    "gpu=0\n",
    "#Method options\n",
    "train_iteration=256\n",
    "out='result'\n",
    "alpha=0.75\n",
    "lambda_u=75\n",
    "T=0.5\n",
    "ema_decay=0.999\n",
    "\n",
    "n_out = 2\n",
    "model_arq = models.resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "np.random.seed(manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_one_batch(loader, loader_iter):\n",
    "    try:\n",
    "        return loader.one_batch()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return next(loader_iter)\n",
    "        except StopIteration:\n",
    "            loader_iter = iter(loader)\n",
    "            return next(loader_iter)\n",
    "\n",
    "def train(labeled_trainloader, unlabeled_trainloader, model, n_out, optimizer, ema_optimizer, criterion, preprocess, epoch, use_cuda, writer, writer_prefix, step, fastai_metrics=[]):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    ws = AverageMeter()\n",
    "    end = time.time()\n",
    "    writer_metrics = {\n",
    "        ('time','batch_time'): batch_time,\n",
    "        ('time','data_time'): data_time,\n",
    "        ('losses','loss'): losses,\n",
    "        ('losses','losses_x'): losses_x,\n",
    "        ('losses','losses_u'): losses_u,\n",
    "        ('losses','ws'): ws,\n",
    "    }\n",
    "    add_metrics = {}\n",
    "    for metric in fastai_metrics:\n",
    "        metric.reset()\n",
    "        add_metrics[('add_metric', metric.name)] = metric\n",
    "\n",
    "    labeled_train_iter = iter(labeled_trainloader)\n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(range(train_iteration), desc=f'Epoch {epoch}')\n",
    "    for batch_idx in pbar:\n",
    "        \n",
    "        inputs, targets = preprocess(labeled_trainloader, unlabeled_trainloader, model, n_out, labeled_train_iter=labeled_train_iter, unlabeled_train_iter=unlabeled_train_iter)\n",
    "\n",
    "        logits = model(inputs)\n",
    "\n",
    "        losses_dict = criterion(logits, targets, batch_size, epoch+batch_idx/train_iteration)\n",
    "\n",
    "        # record loss\n",
    "        losses.update(losses_dict['loss'].item(), batch_size)\n",
    "        losses_x.update(losses_dict['Lx'].item(), batch_size)\n",
    "        losses_u.update(losses_dict['Lu'].item(), batch_size)\n",
    "        ws.update(losses_dict['w'], batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        losses_dict['loss'].backward()\n",
    "        optimizer.step()\n",
    "        ema_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # store predictions on batch\n",
    "        preds = logits[:batch_size].argmax(dim=-1)\n",
    "        targs = targets[:batch_size].argmax(dim=-1)\n",
    "        for (metric_group, metric_name), metric in add_metrics.items():\n",
    "            metric.accum_values(preds, targs)\n",
    "\n",
    "        # write on tensorboardX\n",
    "        step += 1\n",
    "        for (metric_group, metric_name), metric in writer_metrics.items():\n",
    "            writer.add_scalar(f'{metric_group}/{writer_prefix}_{metric_name}', metric.val, step)\n",
    "        for (metric_group, metric_name), metric in add_metrics.items():\n",
    "            writer.add_scalar(f'{metric_group}/{writer_prefix}_{metric_name}', metric.value, step)\n",
    "\n",
    "        # plot progress# plot progress\n",
    "        bar_metrics = {}\n",
    "        for (metric_group, metric_name), metric in add_metrics.items():\n",
    "            bar_metrics[metric_name] = metric.value\n",
    "        pbar.set_postfix(ordered_dict=bar_metrics, data_time=data_time.avg, batch_time=batch_time.avg, loss=losses.avg, loss_x=losses_x.avg, loss_u=losses_u.avg, w=ws.avg)\n",
    "\n",
    "    return (losses.avg, losses_x.avg, losses_u.avg,)\n",
    "\n",
    "def validate(valloader, model, criterion, epoch, use_cuda, mode, writer, writer_prefix, step, fastai_metrics=[]):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    # top5 = AverageMeter()\n",
    "    writer_metrics = {\n",
    "        ('time','batch_time'): batch_time,\n",
    "        ('time','data_time'): data_time,\n",
    "        ('losses','loss'): losses,\n",
    "        ('accuracy','acc'): top1,\n",
    "    }\n",
    "    add_metrics = {}\n",
    "    for metric in fastai_metrics:\n",
    "        metric.reset()\n",
    "        add_metrics[('add_metric', metric.name)] = metric\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    try:\n",
    "        pbar = tqdm(range(len(valloader)), desc=mode)\n",
    "    except TypeError:\n",
    "        pbar = tqdm(range(len(valloader.train)), desc=mode)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in pbar:\n",
    "\n",
    "            inputs, targets = get_one_batch(valloader, iter(valloader))\n",
    "\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            \n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            \n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Transform to one dimensional if needed\n",
    "            if len(targets.size()) > 1:\n",
    "                targets = targets.argmax(dim=-1)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs, targets)\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            for (metric_group, metric_name), metric in add_metrics.items():\n",
    "                metric.accum_values(preds, targets)\n",
    "            \n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            # top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar_metrics = {}\n",
    "            for (metric_group, metric_name), metric in add_metrics.items():\n",
    "                bar_metrics[metric_name] = metric.value\n",
    "            pbar.set_postfix(ordered_dict=bar_metrics, top1=top1.avg, loss=losses.avg, data_time=data_time.avg, batch_time=batch_time.avg)\n",
    "\n",
    "    # write on tensorboardX\n",
    "    for (metric_group, metric_name), metric in writer_metrics.items():\n",
    "        writer.add_scalar(f'{metric_group}/{writer_prefix}_{metric_name}', metric.val, step)\n",
    "    for (metric_group, metric_name), metric in add_metrics.items():\n",
    "        writer.add_scalar(f'{metric_group}/{writer_prefix}_{metric_name}', metric.value, step)\n",
    "\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint=out, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def linear_rampup(current, rampup_length=epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "def preprocess(labeled_trainloader, unlabeled_trainloader, model, n_out, **kwargs):\n",
    "    if 'labeled_train_iter' in kwargs:\n",
    "        labeled_train_iter = kwargs['labeled_train_iter']\n",
    "    else:\n",
    "        labeled_train_iter = iter(labeled_trainloader)\n",
    "        \n",
    "    if 'unlabeled_train_iter' in kwargs:\n",
    "        unlabeled_train_iter = kwargs['unlabeled_train_iter']\n",
    "    else:\n",
    "        unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    inputs_x, targets_x = get_one_batch(labeled_trainloader, labeled_train_iter)\n",
    "\n",
    "    (inputs_u, inputs_u2), _ = get_one_batch(unlabeled_trainloader, unlabeled_train_iter)\n",
    "\n",
    "    # # measure data loading time\n",
    "    # data_time.update(time.time() - end)\n",
    "\n",
    "    batch_size = inputs_x.size(0)\n",
    "\n",
    "    # Transform label to one-hot if needed\n",
    "    if len(targets_x.size()) == 1:\n",
    "        targets_x = torch.zeros(batch_size, n_out).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "\n",
    "    if use_cuda:\n",
    "        inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "        inputs_u = inputs_u.cuda()\n",
    "        inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute guessed labels of unlabel samples\n",
    "        outputs_u = model(inputs_u)\n",
    "        outputs_u2 = model(inputs_u2)\n",
    "        p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "        pt = p**(1/T)\n",
    "        targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "        targets_u = targets_u.detach()\n",
    "\n",
    "    # mixup\n",
    "    all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
    "    all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "    \n",
    "    l = np.random.beta(alpha, alpha)\n",
    "\n",
    "    l = max(l, 1-l)\n",
    "\n",
    "    idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "    input_a, input_b = all_inputs, all_inputs[idx]\n",
    "    target_a, target_b = all_targets, all_targets[idx]\n",
    "\n",
    "    mixed_input = l * input_a + (1 - l) * input_b\n",
    "    mixed_target = l * target_a + (1 - l) * target_b\n",
    "\n",
    "    # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
    "    mixed_input = interleave(mixed_input, batch_size)\n",
    "\n",
    "    return mixed_input, mixed_target\n",
    "\n",
    "class SemiLoss(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, logits, targets, batch_size, epoch):\n",
    "\n",
    "        # put interleaved samples back\n",
    "        logits = de_interleave(logits, batch_size)\n",
    "\n",
    "        Lx = self.Lx_criterion(logits, targets)\n",
    "        Lu = self.Lu_criterion(logits, targets)\n",
    "        w = self.w_scheduling(epoch)\n",
    "\n",
    "        loss = Lx + w * Lu\n",
    "\n",
    "        return {'loss': loss, 'Lx': Lx, 'Lu': Lu, 'w': w}\n",
    "\n",
    "class MixMatchLoss(SemiLoss):\n",
    "    def __init__(self, weight=1):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def Lx_criterion(self, logits, targets):\n",
    "        logits_x = logits[:batch_size]\n",
    "        targets_x = targets[:batch_size]\n",
    "        \n",
    "        return -torch.mean(torch.sum(self.weight * F.log_softmax(logits_x, dim=1) * targets_x, dim=1))\n",
    "\n",
    "    def Lu_criterion(self, logits, targets):\n",
    "        logits_u = logits[batch_size:]\n",
    "        targets_u = targets[batch_size:]\n",
    "        \n",
    "        probs_u = torch.softmax(logits_u, dim=1)\n",
    "        \n",
    "        return torch.mean((probs_u - targets_u)**2)\n",
    "\n",
    "    def w_scheduling(self, epoch):\n",
    "        return lambda_u * linear_rampup(epoch)\n",
    "\n",
    "class WeightEMA(object):\n",
    "    def __init__(self, model, ema_model, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.alpha = alpha\n",
    "        self.params = list(model.state_dict().values())\n",
    "        self.ema_params = list(ema_model.state_dict().values())\n",
    "        self.wd = 0.02 * lr\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if ema_param.dtype==torch.float32:\n",
    "                ema_param.mul_(self.alpha)\n",
    "                ema_param.add_(param * one_minus_alpha)\n",
    "                # customized weight decay\n",
    "                param.mul_(1 - self.wd)\n",
    "\n",
    "def interleave(x, bt):\n",
    "    s = list(x.shape)\n",
    "    return torch.reshape(torch.transpose(x.reshape([-1, bt] + s[1:]), 1, 0), [-1] + s[1:])\n",
    "\n",
    "def de_interleave(x, bt):\n",
    "    s = list(x.shape)\n",
    "    return torch.reshape(torch.transpose(x.reshape([bt, -1] + s[1:]), 1, 0), [-1] + s[1:])\n",
    "\n",
    "def TestColSplitter(col='Dataset'):\n",
    "    \"Split `items` (supposed to be a dataframe) by value in `col`\"\n",
    "    def _inner(o):\n",
    "        assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n",
    "        train_idx = (o.iloc[:,col] if isinstance(col, int) else o[col]) == 'train'\n",
    "        valid_idx = (o.iloc[:,col] if isinstance(col, int) else o[col]) == 'valid'\n",
    "        test_idx = (o.iloc[:,col] if isinstance(col, int) else o[col]) == 'test'\n",
    "        return IndexSplitter(mask2idxs(train_idx))(o)[1], IndexSplitter(mask2idxs(valid_idx))(o)[1], IndexSplitter(mask2idxs(test_idx))(o)[1]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def normalise(x, mean, std):\n",
    "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
    "    x -= mean\n",
    "    x *= 1.0/std\n",
    "    return x\n",
    "\n",
    "def transpose(x, source='NHWC', target='NCHW'):\n",
    "    return x.transpose([source.index(d) for d in target]) \n",
    "\n",
    "class MixMatchDataset(Dataset):\n",
    "    \"\"\"MixMatch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, suff='', labeled=True, transform=None, size=512):\n",
    "        self.df = df\n",
    "        if labeled:\n",
    "            self.df.iloc[:, 1] = self.df.iloc[:, 1].astype('category').cat.codes\n",
    "        self.root_dir = root_dir\n",
    "        self.suff = suff\n",
    "        self.labeled = labeled\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "\n",
    "        self.data = []\n",
    "        for idx in tqdm(range(len(df)), desc='Init images: '):\n",
    "            img_name = os.path.join(self.root_dir,\n",
    "                                    self.df.iloc[idx, 0] + self.suff)\n",
    "            image = Image.open(img_name)\n",
    "            image = torch_transforms.Resize(self.size)(image)\n",
    "            image = np.array(image)\n",
    "            self.data.append(image)\n",
    "        self.data = np.array(self.data)\n",
    "\n",
    "        self.data_mean = np.mean(self.data, axis=(0,1,2)) # or use ImageNet stats (dividing all by 255)\n",
    "        self.data_std = np.std(self.data, axis=(0,1,2)) # or use ImageNet stats (dividing all by 255)\n",
    "\n",
    "        self.data = normalise(self.data, self.data_mean, self.data_std)\n",
    "        if len(self.data.shape) > 3:\n",
    "            self.data = transpose(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Use this if not all images can fit in-memory when __init__\n",
    "        # img_name = os.path.join(self.root_dir,\n",
    "        #                         self.df.iloc[idx, 0] + self.suff)\n",
    "        # image = Image.open(img_name)\n",
    "        # image = normalise(image, self.data_mean, self.data_std)\n",
    "        # if len(image.shape) > 2:\n",
    "        #     image = transpose(, source='HWC', target='CHW')\n",
    "\n",
    "        image = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            if self.labeled:\n",
    "                image = self.transform(image)\n",
    "                target = torch.as_tensor(self.df.iloc[idx, 1], dtype=torch.long)\n",
    "            else:\n",
    "                image = (self.transform(image), self.transform(image))\n",
    "                target = -1\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(out):\n",
    "    mkdir_p(out)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    PATH_DRIVER = '/content/gdrive/My Drive/'\n",
    "    DATA_FOLDER = 'Dataset/'\n",
    "else:\n",
    "    PATH_DRIVER = ''\n",
    "    DATA_FOLDER = 'C:/Users/Lluis/Desktop/Machine Learning/radiology_ai/data/'\n",
    "\n",
    "PATH_PREFIX = os.path.join(PATH_DRIVER, DATA_FOLDER, '')\n",
    "raw_folder = PATH_PREFIX + 'DICOMS'\n",
    "organize_folder = PATH_PREFIX + 'pending_classification'\n",
    "preprocess_folder = PATH_PREFIX + 'preprocess'\n",
    "\n",
    "df = pd.read_excel(os.path.join(PATH_PREFIX, 'all.xlsx'), dtype={'ID':'string','Target':'string'})\n",
    "unlabel_df = df[['ID','Target']][df['Target'].isnull()].reset_index(drop=True)\n",
    "label_df = df[['ID','Target']][df['Target'].notnull()].reset_index(drop=True)\n",
    "train_df, test_df = train_test_split(label_df, test_size=0.15, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, shuffle=True)\n",
    "label_df.loc[train_df.index, 'Dataset'] = 'train'\n",
    "label_df.loc[val_df.index, 'Dataset'] = 'valid'\n",
    "label_df.loc[test_df.index, 'Dataset'] = 'test'\n",
    "\n",
    "# Data\n",
    "print(f'==> Preparing dataset')\n",
    "transform_train = torch_transforms.Compose([\n",
    "    torch_transforms.ToPILImage(),\n",
    "    # torch_transforms.Resize(512),\n",
    "    torch_transforms.RandomCrop(256),\n",
    "    torch_transforms.RandomHorizontalFlip(),\n",
    "    torch_transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fastai_transform = [\n",
    "    RandomResizedCropGPU(256),\n",
    "    Flip(),\n",
    "    Normalize()\n",
    "]\n",
    "\n",
    "labeled_loader = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), MultiCategoryBlock),\n",
    "    get_x=ColReader('ID', pref=preprocess_folder+'/', suff='.png'), \n",
    "    get_y=ColReader('Target'),\n",
    "    splitter=TestColSplitter(col='Dataset'),\n",
    "    item_tfms=Resize(512),\n",
    "    batch_tfms=fastai_transform,\n",
    ").dataloaders(label_df, bs=batch_size, num_workers=0, shuffle_train=True, drop_last=True)\n",
    "\n",
    "unlabeled_trainloader = data.DataLoader(\n",
    "    MixMatchDataset(unlabel_df, root_dir=preprocess_folder, suff='.png', labeled=False, transform=transform_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Model\n",
    "print(\"==> creating model\")\n",
    "\n",
    "def create_model(model_arq, n_out, pretrained=True, n_in=1, ema=False):\n",
    "    model = create_cnn_model(model_arq, n_out=n_out, cut=None, pretrained=pretrained, n_in=n_in)\n",
    "    model = model.cuda()\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model(model_arq, n_out, pretrained=True, n_in=1)\n",
    "ema_model = create_model(model_arq, n_out, pretrained=True, n_in=1, ema=True)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "class_weight = compute_class_weight(class_weight='balanced', classes=train_df['Target'].unique(), y=train_df['Target'])\n",
    "class_weight = torch.as_tensor(class_weight)\n",
    "if use_cuda:\n",
    "    class_weight = class_weight.cuda()\n",
    "\n",
    "train_criterion = MixMatchLoss(weight=class_weight)\n",
    "criterion = train_criterion.Lx_criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "ema_optimizer= WeightEMA(model, ema_model, alpha=ema_decay)\n",
    "start_epoch = 0\n",
    "\n",
    "# Resume\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isfile(resume), 'Error: no checkpoint directory found!'\n",
    "    out = os.path.dirname(resume)\n",
    "    checkpoint = torch.load(resume)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    ema_model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "writer = SummaryWriter(out, flush_secs=10)\n",
    "f1_score = F1ScoreMulti(average='macro')\n",
    "precision = PrecisionMulti(average='macro')\n",
    "recall = RecallMulti(average='macro')\n",
    "fastai_metrics = [f1_score, precision, recall]\n",
    "step = 0\n",
    "test_accs = []\n",
    "# Train and val\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    train_loss, train_loss_x, train_loss_u = train(labeled_loader[0], unlabeled_trainloader, model, n_out, optimizer, ema_optimizer, train_criterion, preprocess, epoch, use_cuda, writer=writer, writer_prefix='train', step=step, fastai_metrics=fastai_metrics)\n",
    "\n",
    "    step = train_iteration * (epoch + 1)\n",
    "\n",
    "    _, train_acc = validate(labeled_loader[0], ema_model, criterion, epoch, use_cuda, mode='Train Stats', writer=writer, writer_prefix='train_eval', step=step, fastai_metrics=fastai_metrics)\n",
    "    val_loss, val_acc = validate(labeled_loader[1], ema_model, criterion, epoch, use_cuda, mode='Valid Stats', writer=writer, writer_prefix='valid', step=step, fastai_metrics=fastai_metrics)\n",
    "    test_loss, test_acc = validate(labeled_loader[2], ema_model, criterion, epoch, use_cuda, mode='Test Stats', writer=writer, writer_prefix='test', step=step, fastai_metrics=fastai_metrics)\n",
    "\n",
    "    # save model\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(val_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'ema_state_dict': ema_model.state_dict(),\n",
    "            'acc': val_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "    test_accs.append(test_acc)\n",
    "writer.close()\n",
    "\n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n",
    "print('Mean acc:')\n",
    "print(np.mean(test_accs[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.interpret import ClassificationInterpretation\n",
    "\n",
    "interp = ClassificationInterpretation(dl, inputs, preds, targs, decoded, losses)\n"
   ]
  }
 ]
}