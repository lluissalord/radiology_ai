{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37864bitradiologyaiconda1d6e040f57e346eb9ba4a0a95c0ad7a6",
   "display_name": "Python 3.7.8 64-bit ('radiology_ai': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define if we are on Colab and mount drive { display-mode: \"form\" }\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (COLAB ONLY) Clone GitHub repo { display-mode: \"form\" }\n",
    "\n",
    "if IN_COLAB:\n",
    "  !git clone https://github.com/lluissalord/radiology_ai.git\n",
    "\n",
    "  %cd radiology_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup environment and Colab general variables { display-mode: \"form\" }\n",
    "# %%capture\n",
    "%run colab_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Move images from Drive to temporary folder here to be able to train models { display-mode: \"form\" }\n",
    "# %%capture\n",
    "%run move_raw_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.callback import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "from fastai.medical.imaging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from preprocessing import *\n",
    "\n",
    "from APL_losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastprogress.fastprogress import NBMasterBar, NBProgressBar\n",
    "\n",
    "# master_bar, progress_bar = NBMasterBar, NBProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RESIZE = 256\n",
    "RANDOM_RESIZE_CROP = 256\n",
    "RANDOM_MIN_SCALE = 0.5\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "N_TRAIN = None\n",
    "N_SAMPLES_BIN = 50 # None\n",
    "\n",
    "HIST_CLIPPING = True\n",
    "HIST_CLIPPING_CUT_MIN = 5.\n",
    "HIST_CLIPPING_CUT_MAX = 99.\n",
    "\n",
    "KNEE_LOCALIZER = True\n",
    "CLAHE_SCALED = True\n",
    "HIST_SCALED = False\n",
    "HIST_SCALED_SELF = False\n",
    "\n",
    "USE_SAVED_MODEL = False\n",
    "SAVE_MODEL = False\n",
    "\n",
    "MODEL = resnet18\n",
    "MODEL_VERSION = 0\n",
    "MODEL_SAVE_NAME = f'{MODEL.__name__}_v{MODEL_VERSION}.pkl'\n",
    "MODEL_SAVE_PATH = os.path.join(models_folder, MODEL_SAVE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_check_DICOM_dict = {\n",
    "    'ap': {\n",
    "        'Modality': ['CR', 'DR', 'DX'],\n",
    "        'SeriesDescription': ['RODILLA AP', 'TIBIA AP DIRECTO', 'Rodilla AP', 'rodilla AP', 'W098bDER Rodilla a.p.', 'T098aDER Rodilla a.p.', 'rodilla 1P AP', 'xeonllo DEREITO AP', 'xeonllo ESQUERDO AP'],\n",
    "        'BodyPartExamined': ['LOWER LIMB', 'KNEE'],\n",
    "        'function': lambda row: row.Rows/row.Columns >= 0.83,\n",
    "    },\n",
    "    # 'lat': {\n",
    "    #     'Modality': ['CR', 'DR', 'DX'],\n",
    "    #     'SeriesDescription': ['RODILLA LAT', 'TIBIA LAT DIRECTO', 'RODILLA LAT EN CARGA', 'T Rodilla lat', 'rodilla LAT', 'rodilla  LAT', 'W098bDER Rodilla lat.', 'T098aDER Rodilla lat', 'rodilla 1P LAT', 'xeonllo DEREITO LAT', 'xeonllo ESQUERDO LAT', 'TOBILLO EN CARGA LAT', 'PIE LAT EN CARGA', 'rodilla LAT dcha', 'rodilla LAT izda'],\n",
    "    #     'BodyPartExamined': ['LOWER LIMB', 'KNEE']\n",
    "    # },\n",
    "    # 'two': {\n",
    "    #     'Modality': ['CR', 'DR', 'DX'],\n",
    "    #     'SeriesDescription': ['RODILLAS AP', 'rodilla AP y LAT', 'ambas rodillas AP', 'ambas rodillas LAT', 'rodilla (telemando) AP y LAT', 'rodilla AP y LAT', 'Rodillas LAT', 'Rodilla AP y LAT', 'ambolos dous xeonllos AP', 'ambolos dous xeonllos LAT', 'rodilla seriada', 'Rodillas AP', 'Rodillas LAT'],\n",
    "    #     'BodyPartExamined': ['LOWER LIMB', 'KNEE']\n",
    "    # },\n",
    "    # 'other': {\n",
    "    #     'Modality': ['CR', 'DR', 'DX'],\n",
    "    #     'BodyPartExamined': ['THORAX', 'UPPER LIMB', 'KNEE STANDING',\n",
    "    #    'RIBS', 'HAND', 'HIP', 'PIE EN CARGA', 'FOOT', 'ANKLE',\n",
    "    #    'ELBOW', 'PELVIS', 'LSPINE', 'CSPINE']\n",
    "    # }\n",
    "}\n",
    "\n",
    "targets = list(all_check_DICOM_dict.keys()) + ['other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_raw_path = os.path.join(PATH_PREFIX, 'metadata_raw.csv')\n",
    "metadata_df = pd.read_csv(metadata_raw_path)\n",
    "metadata_df.fname = metadata_df.fname.apply(\n",
    "    lambda x: os.path.normpath(\n",
    "        os.path.join(\n",
    "            raw_preprocess_folder,\n",
    "            os.path.split(x)[-1] + '.png'\n",
    "        )\n",
    "    )\n",
    "    .replace(os.sep, '/')\n",
    ")\n",
    "\n",
    "metadata_labels_path = os.path.join(PATH_PREFIX, 'metadata_labels.csv')\n",
    "metadata_labels = pd.read_csv(metadata_labels_path)\n",
    "reviewed_labels = metadata_labels[metadata_labels['Prob'].isnull()].rename({'Path': 'fname'}, axis=1)\n",
    "reviewed_labels = reviewed_labels.set_index('fname')\n",
    "\n",
    "# Define which column to use as the prediction\n",
    "if 'Final_pred' in reviewed_labels.columns:\n",
    "    pred_col = 'Final_pred'\n",
    "else:\n",
    "    pred_col = 'Pred'\n",
    "\n",
    "# Initialize lists containing the filenames for each class\n",
    "all_fnames = {}\n",
    "\n",
    "for label, check_DICOM_dict in all_check_DICOM_dict.items():\n",
    "    # Check DICOM which according to the metadata should be that label\n",
    "    match_df = df_check_DICOM(metadata_df, check_DICOM_dict)\n",
    "    \n",
    "    # Remove cases that have been reviewed and selected as DIFFERENT from the current label\n",
    "    match_df = match_df.merge(reviewed_labels[reviewed_labels[pred_col] != label], how='left', left_on='fname', right_index=True)\n",
    "    match_df = match_df[match_df[pred_col].isnull()]\n",
    "\n",
    "    # Add cases that have been reviewed and selected as EQUAL from the current label\n",
    "    match_df = pd.concat(\n",
    "        [\n",
    "            reviewed_labels[reviewed_labels[pred_col] == label].reset_index(),\n",
    "            match_df\n",
    "        ]\n",
    "    ).drop_duplicates('fname').reset_index(drop=True)\n",
    "    all_fnames[label] = L(list(match_df.fname))\n",
    "\n",
    "# Set as raw filenames all the ones in the metadata DataFrame\n",
    "raw_fnames = L(list(metadata_df.fname))\n",
    "\n",
    "# Filter on the filenames to not include undesired files\n",
    "raw_fnames = L(filter_fnames(raw_fnames, metadata_raw_path))\n",
    "\n",
    "# Label the rest of images as other\n",
    "other_fnames = copy(raw_fnames)\n",
    "for label, fnames in all_fnames.items():\n",
    "    other_fnames = L(set(other_fnames.map(lambda path: str(path).replace(os.sep, '/'))) - set(fnames))\n",
    "\n",
    "# Filter on the filenames to not include undesired files\n",
    "other_fnames = filter_fnames(other_fnames, metadata_raw_path)\n",
    "all_fnames['other'] = L(other_fnames)\n",
    "\n",
    "# Select the corresponding part for training\n",
    "if N_TRAIN is None:\n",
    "    fnames = raw_fnames\n",
    "else:\n",
    "    fnames = random.choices(raw_fnames, k=N_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets)\n",
    "[len(all_fnames[label]) for label in all_fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trying to not use oversampling due to issues on relabeling data and because with 2 labels is already somehow balanced\n",
    "\n",
    "# # Oversampling of all classes to meet the biggest one or reach max_n_times its own size\n",
    "# max_samples = max([len(fnames) for _, fnames in all_fnames.items()])\n",
    "# max_n_times = 4\n",
    "# for label, fnames in all_fnames.items():\n",
    "#     k = min(max_samples-len(fnames), max_n_times * len(fnames))\n",
    "#     all_fnames[label] = all_fnames[label] + random.choices(all_fnames[label], k=k)\n",
    "\n",
    "# print(targets)\n",
    "# [len(all_fnames[label]) for label in all_fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with the filenames and the corresponding label\n",
    "labels_concat = []\n",
    "for label, fnames in all_fnames.items():\n",
    "    label_df = pd.DataFrame(list(fnames), columns=['fname'])\n",
    "    label_df['Target'] = label\n",
    "    labels_concat.append(label_df)\n",
    "\n",
    "labels_df = pd.concat(labels_concat).set_index('fname', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tfms = []\n",
    "\n",
    "if HIST_CLIPPING:\n",
    "    item_tfms.append(XRayPreprocess(PIL_cls=PILImageBW, cut_min=HIST_CLIPPING_CUT_MIN, cut_max=HIST_CLIPPING_CUT_MAX, np_input=len(item_tfms) > 0, np_output=True))\n",
    "\n",
    "if KNEE_LOCALIZER:\n",
    "    item_tfms.append(KneeLocalizer(KNEE_SVM_MODEL_PATH, PIL_cls=PILImageBW, resize=TRAIN_RESIZE, np_input=len(item_tfms) > 0, np_output=True))\n",
    "\n",
    "batch_tfms = [\n",
    "    Flip(),\n",
    "    *aug_transforms(\n",
    "        pad_mode=PadMode.Zeros,\n",
    "    ),\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP, min_scale=RANDOM_MIN_SCALE),\n",
    "    Normalize()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram scaling DICOM on the fly\n",
    "\n",
    "if CLAHE_SCALED:\n",
    "    item_tfms.append(CLAHE_Transform(PIL_cls=PILImageBW, grayscale=True, np_input=len(item_tfms) > 0, np_output=False))\n",
    "elif HIST_SCALED:\n",
    "    if HIST_SCALED_SELF:\n",
    "        bins = None\n",
    "    else:\n",
    "        # bins = init_bins(fnames=L(list(final_df['Original'].values)), n_samples=100)\n",
    "        all_valid_raw_preprocess = pd.concat([pd.Series(unlabel_all_df.index), label_df['Raw_preprocess']])\n",
    "        bins = init_bins(L([fname for labels,fnames in all_fnames.items() for fname in fnames]), n_samples=N_SAMPLES_BIN, isDCM=False)\n",
    "    # item_tfms.append(HistScaled(bins))\n",
    "    item_tfms.append(HistScaled_all(bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataBlock(\n",
    "    blocks=(ImageBlock(PILImageBW), CategoryBlock),\n",
    "    get_x=ColReader('fname'),\n",
    "    get_y=ColReader('Target'),\n",
    "    splitter=RandomSplitter(0.2),\n",
    "    item_tfms=item_tfms,\n",
    "    batch_tfms=batch_tfms,\n",
    ").dataloaders(labels_df, bs=BATCH_SIZE, num_workers=0, shuffle_train=True, drop_last=True)\n",
    "dls.show_batch(max_n=25, cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = None\n",
    "\n",
    "loss_func = NCEandRCE(1, 1, len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks that will be used during training\n",
    "callback_fns = [\n",
    "        MixUp(),\n",
    "        # partial(OverSamplingCallback),\n",
    "        # ShowGraphCallback(),\n",
    "        EarlyStoppingCallback(monitor='val_loss', min_delta=0.05, patience=2),\n",
    "    ]\n",
    "\n",
    "# Adapt metrics depending on the number of labels\n",
    "if len(targets) == 2:\n",
    "    average = 'binary'\n",
    "    roc_auc = RocAucBinary()\n",
    "else:\n",
    "    average = 'macro'\n",
    "    roc_auc = RocAuc()\n",
    "\n",
    "f1_score = F1Score(average=average)\n",
    "precision = Precision(average=average)\n",
    "recall = Recall(average=average)\n",
    "learn = cnn_learner(\n",
    "    dls,\n",
    "    resnet18,\n",
    "    metrics=[\n",
    "        error_rate,\n",
    "        roc_auc,\n",
    "        f1_score,\n",
    "        precision,\n",
    "        recall\n",
    "    ],\n",
    "    loss_func=loss_func,\n",
    "    callback_fns=callback_fns,\n",
    "    config={'n_in': 1}\n",
    ")\n",
    "\n",
    "# Regularization by using float precision of 16 bits\n",
    "# This helps to not overfit because is more difficult to \"memorize\" images, but enough to learn\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SAVED_MODEL:\n",
    "    model_load = create_model(MODEL, len(targets))\n",
    "    opt_load = copy(learn.opt)\n",
    "\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "\n",
    "    load_model(file=MODEL_SAVE_PATH, model=model_load, opt=opt_load, device=0)\n",
    "    learn.model = model_load\n",
    "    learn.opt = opt_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learn.fine_tune(5, 0.025, freeze_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn.show_results(max_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL:\n",
    "    save_model(file=os.path.join('models', MODEL_SAVE_NAME), model=learn.model, opt=learn.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use too much RAM and the session is not capable of handle it\n",
    "# interp = Interpretation.from_learner(learn)\n",
    "# losses, idx = interp.top_losses()\n",
    "# interp.plot_top_losses(25, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating DataLoader and select the paths that will be used for inference from raw_fnames\n",
    "raw_dls = DataBlock(\n",
    "    blocks=(ImageBlock(PILImageBW)),\n",
    "    get_x=ColReader('fname'),\n",
    "    item_tfms=item_tfms,\n",
    ").dataloaders(pd.DataFrame(list(raw_fnames),columns=['fname']), bs=BATCH_SIZE, num_workers=0, shuffle_train=True, drop_last=True)\n",
    "\n",
    "# paths = raw_dls.train.items + raw_dls.valid.items\n",
    "paths = list(raw_dls.train.items.iloc[:,0].values) + list(raw_dls.valid.items.iloc[:,0].values)\n",
    "labels = [labels_df.loc[path, 'Target'] if type(labels_df.loc[path, 'Target']) is not pd.Series else labels_df.loc[path, 'Target'][0] for path in tqdm(paths, desc='Matching labels')]\n",
    "\n",
    "# Add DataSet from paths to the Test set of the learner\n",
    "dl = learn.dls.test_dl(paths)\n",
    "\n",
    "# Calculate predictions and probabilities\n",
    "preds, _ = learn.tta(dl=dl)\n",
    "# preds, _ = learn.get_preds(dl=dl)\n",
    "max_probs, targs = preds.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_threshold = {\n",
    "    'Correct_label': 0.95,\n",
    "    'Wrong_label': 0.95,\n",
    "}\n",
    "\n",
    "metadata_labels_path = os.path.join(PATH_PREFIX, 'metadata_labels.csv')\n",
    "metadata_labels = pd.read_csv(metadata_labels_path)\n",
    "reviewed_labels = metadata_labels[metadata_labels['Prob'].isnull()].rename({'Path': 'fname'}, axis=1)\n",
    "reviewed_labels = reviewed_labels.set_index('fname')\n",
    "\n",
    "data = {\n",
    "    'Path': [],\n",
    "    'Label': [],\n",
    "    'Raw_pred': [],\n",
    "    'Pred': [],\n",
    "    'Prob': [],\n",
    "}\n",
    "to_be_reviewed = []\n",
    "for label, prob, targ, path_str in tqdm(zip(labels, max_probs, targs, paths), total=len(labels)):\n",
    "    path = Path(path_str)\n",
    "    raw_pred = targets[targ]\n",
    "    \n",
    "    # Check if already reviewed\n",
    "    try:\n",
    "        review = reviewed_labels.loc[path_str]\n",
    "\n",
    "        # Set current data if reviews\n",
    "        pred = review['Final_pred']\n",
    "        prob = np.NaN\n",
    "    except KeyError:\n",
    "        # Set prob and pred according to the thresholds\n",
    "        prob = float(prob)\n",
    "\n",
    "        # Case of confidence on itself to predict same labels as target\n",
    "        if label == targets[targ]:\n",
    "            if prob >= class_threshold['Correct_label']:\n",
    "                pred = raw_pred\n",
    "            else:\n",
    "                pred = 'Unsure_' + targets[targ] + '_' + label\n",
    "                to_be_reviewed.append((path, targets[targ], label, prob))\n",
    "\n",
    "        # Confidence on wrong labelling\n",
    "        else:\n",
    "            if prob >= class_threshold['Wrong_label']:\n",
    "                pred = raw_pred\n",
    "            else:\n",
    "                pred = 'Unsure_' + targets[targ] + '_' + label\n",
    "                to_be_reviewed.append((path, targets[targ], label, prob))\n",
    "\n",
    "    data['Path'].append(os.path.normpath(path).replace(os.sep, '/'))\n",
    "    data['Label'].append(label)\n",
    "    data['Raw_pred'].append(raw_pred)\n",
    "    data['Pred'].append(pred)\n",
    "    data['Prob'].append(prob)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(metadata_labels_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _open_thumb(fn, h, w): return Image.open(fn).to_thumb(h, w).convert('RGBA')\n",
    "\n",
    "class ImagesCleanerDefaultPred(ImagesCleaner):\n",
    "    \"A widget that displays all images in `fns` along with a `Dropdown` with default value the prediction\"\n",
    "\n",
    "    def set_fns(self, fns, preds, labels, probs):\n",
    "        self.fns = L(fns)[:self.max_n]\n",
    "        # ims = parallel(_open_thumb, self.fns, h=self.height, w=self.width, progress=False,\n",
    "        #                n_workers=min(len(self.fns)//10,defaults.cpus))\n",
    "        ims = [_open_thumb(fn, h=self.height, w=self.width) for fn in self.fns]\n",
    "        self.widget.children = [\n",
    "            VBox([\n",
    "                Label(f'{pred}/{label}/{prob:.4f}'),\n",
    "                widget(im, height=f'{self.height}px'),\n",
    "                Dropdown(options=self.opts, layout={'width': 'max-content'}, value=pred)\n",
    "            ]) for im, pred, label, prob in zip(ims,preds,labels,probs)\n",
    "        ]\n",
    "\n",
    "    def values(self): return L(self.widget.children).itemgot(-1).attrgot('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unsure with lowest probability\n",
    "df_to_review = df[(~df['Pred'].isin(targets)) & (df['Prob'].notnull())].sort_values(['Raw_pred', 'Prob']).iloc[:100]\n",
    "\n",
    "# # Check the OTHER cases which the model is totally sure and are also confirmed by metadata\n",
    "# df_to_review = df_rev[(df_rev['Label'] == 'other') & (df_rev['Pred'] == 'other') & (df['Prob'].notnull())].sort_values('Prob', ascending=False).iloc[:100]\n",
    "\n",
    "# # Check the AP cases which the model is totally sure and are also confirmed by metadata\n",
    "# df_to_review = df_rev[(df_rev['Label'] == 'ap') & (df_rev['Pred'] == 'ap') & (df['Prob'].notnull())].sort_values('Prob', ascending=False).iloc[:100]\n",
    "\n",
    "w = ImagesCleanerDefaultPred(targets,  max_n=len(df_to_review.index))\n",
    "w.set_fns(\n",
    "    list(df_to_review['Path']),\n",
    "    list(df_to_review['Raw_pred']),\n",
    "    list(df_to_review['Label']),\n",
    "    list(df_to_review['Prob'])\n",
    ")\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "df['Final_pred'] = df['Pred']\n",
    "for i, pred in w.change():\n",
    "    idx = df_to_review.iloc[i].name\n",
    "    df.loc[idx, 'Final_pred'] = pred\n",
    "    df.loc[idx, 'Prob'] = np.nan\n",
    "\n",
    "    # Update label image if required\n",
    "    path =  Path(df.loc[idx, 'Path'])\n",
    "    if path.parent.name != pred:\n",
    "        labels_df.loc[path, 'Target'] = pred\n",
    "        labels_df.loc[path, 'fname'] = path\n",
    "\n",
    "df.to_csv(metadata_labels_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}