{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define if we are on Colab and mount drive { display-mode: \"form\" }\n",
    "run_params = {}\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  run_params['IN_COLAB'] = True\n",
    "except:\n",
    "  run_params['IN_COLAB'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (COLAB ONLY) Clone GitHub repo { display-mode: \"form\" }\n",
    "\n",
    "if run_params['IN_COLAB']:\n",
    "  !git clone https://github.com/lluissalord/radiology_ai.git\n",
    "\n",
    "  %cd radiology_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup environment and Colab general variables { display-mode: \"form\" }\n",
    "%%capture\n",
    "%run colab_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Move images from Drive to temporary folder here to be able to train models { display-mode: \"form\" }\n",
    "%%capture\n",
    "%run move_raw_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.callback import *\n",
    "from fastai.data.block import *\n",
    "from fastai.data.transforms import *\n",
    "from fastai.medical.imaging import *\n",
    "from fastai.vision.data import *\n",
    "from fastai.vision.learner import *\n",
    "from fastai.vision.augment import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.organize import *\n",
    "from utils.misc import *\n",
    "\n",
    "from preprocessing.transforms import *\n",
    "# from preprocessing.dicom import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params['TEST_SIZE'] = 0.15\n",
    "run_params['VALID_SIZE'] = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params['BATCH_SIZE'] = 32\n",
    "\n",
    "run_params['RESIZE'] = 512\n",
    "\n",
    "\n",
    "run_params['HIST_CLIPPING'] = True\n",
    "run_params['HIST_CLIPPING_CUT_MIN'] = 5.\n",
    "run_params['HIST_CLIPPING_CUT_MAX'] = 99.\n",
    "\n",
    "run_params['KNEE_LOCALIZER'] = True\n",
    "run_params['CLAHE_SCALED'] = True\n",
    "run_params['HIST_SCALED'] = False\n",
    "run_params['HIST_SCALED_SELF'] = True\n",
    "\n",
    "run_params['BINARY_CLASSIFICATION'] = True\n",
    "\n",
    "run_params['USE_SAVED_MODEL'] = True\n",
    "run_params['SAVE_MODEL'] = False\n",
    "\n",
    "run_params['MODEL'] = resnet18\n",
    "run_params['MODEL_VERSION'] = 0\n",
    "run_params['MODEL_DESCRIPTION'] = f'SUP_sz{run_params[\"RESIZE\"]}'\n",
    "run_params['MODEL_SAVE_NAME'] = f'{run_params[\"MODEL\"].__name__}_{run_params[\"MODEL_DESCRIPTION\"]}_v{run_params[\"MODEL_VERSION\"]}.pkl'\n",
    "run_params['MODEL_SAVE_PATH'] = os.path.join(run_params['MODELS_FOLDER'], run_params['MODEL_SAVE_NAME'])\n",
    "\n",
    "run_params['PRETRAINED_MODEL_SAVE_NAME'] = 'resnet18_v0.pkl'\n",
    "run_params['PRETRAINED_MODEL_SAVE_NAME'] = os.path.join(run_params['MODELS_FOLDER'], run_params['PRETRAINED_MODEL_SAVE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params['SEED'] = 42\n",
    "\n",
    "seed_everything(run_params['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "item_tfms = []\n",
    "\n",
    "if run_params['HIST_CLIPPING']:\n",
    "    item_tfms.append(XRayPreprocess(PIL_cls=PILImageBW, cut_min=run_params['HIST_CLIPPING_CUT_MIN'], cut_max=run_params['HIST_CLIPPING_CUT_MAX'], np_input=len(item_tfms) > 0, np_output=True))\n",
    "\n",
    "if run_params['KNEE_LOCALIZER']:\n",
    "    item_tfms.append(KneeLocalizer(run_params['KNEE_SVM_MODEL_PATH'], PIL_cls=PILImageBW, resize=run_params['RESIZE'], np_input=len(item_tfms) > 0, np_output=True))\n",
    "else:\n",
    "    item_tfms.append(Resize(run_params['RESIZE'], method=ResizeMethod.Pad, pad_mode=PadMode.Zeros))\n",
    "\n",
    "# item_tfms.append(RandomResizedCrop(RANDOM_RESIZE_CROP))\n",
    "\n",
    "batch_tfms=[*aug_transforms(), Normalize()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_params['IN_COLAB']:\n",
    "  df = concat_templates(run_params['ORGANIZE_FOLDER'], excel=True)\n",
    "  df.to_excel(\n",
    "      os.path.join(run_params['PATH_PREFIX'], 'all.xlsx'),\n",
    "      index=False\n",
    "  )\n",
    "else:\n",
    "  df = pd.read_excel(os.path.join(run_params['PATH_PREFIX'], 'all.xlsx'), dtype={'ID':'string','Target':'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "relation_df = pd.read_csv(os.path.join(run_params['PATH_PREFIX'], 'relation.csv'))\n",
    "relation_df = relation_df.set_index('Filename')\n",
    "\n",
    "final_df = df.set_index('ID').merge(relation_df, left_index=True, right_index=True)\n",
    "final_df['ID'] = final_df.index.values\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df['Raw_preprocess'] = final_df['Original_Filename'].apply(lambda filename: os.path.join(run_params['RAW_PREPROCESS_FOLDER'], filename + '.png'))\n",
    "\n",
    "unlabel_df = final_df[df['Target'].isnull()].reset_index(drop=True)\n",
    "label_df = final_df[df['Target'].notnull()].reset_index(drop=True)\n",
    "if run_params['BINARY_CLASSIFICATION']:\n",
    "  label_df['Target'] = (label_df['Target'] != '0').astype(int).astype('string')\n",
    "\n",
    "try:\n",
    "  train_df, test_df = train_test_split(label_df, test_size=run_params['TEST_SIZE'], shuffle=True, stratify=label_df['Target'], random_state=run_params['SEED'])\n",
    "except ValueError:\n",
    "  train_df, test_df = train_test_split(label_df, test_size=run_params['TEST_SIZE'], shuffle=True, random_state=run_params['SEED'])\n",
    "\n",
    "try:\n",
    "  train_df, val_df = train_test_split(train_df, test_size=run_params['VALID_SIZE']/(1-run_params['TEST_SIZE']), shuffle=True, stratify=train_df['Target'], random_state=run_params['SEED'])\n",
    "except ValueError:\n",
    "  train_df, val_df = train_test_split(train_df, test_size=run_params['VALID_SIZE']/(1-run_params['TEST_SIZE']), shuffle=True, random_state=run_params['SEED'])\n",
    "\n",
    "label_df.loc[train_df.index, 'Dataset'] = 'train'\n",
    "label_df.loc[val_df.index, 'Dataset'] = 'valid'\n",
    "label_df.loc[test_df.index, 'Dataset'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram scaling DICOM on the fly\n",
    "\n",
    "if run_params['CLAHE_SCALED']:\n",
    "    item_tfms.append(CLAHE_Transform(PIL_cls=PILImageBW, grayscale=True, np_input=len(item_tfms) > 0, np_output=False))\n",
    "elif run_params['HIST_SCALED']:\n",
    "    if run_params['HIST_SCALED_SELF']:\n",
    "        bins = None\n",
    "    else:\n",
    "        # bins = init_bins(fnames=L(list(final_df['Original'].values)), n_samples=100)\n",
    "        bins = init_bins(fnames=L(list(final_df['Raw_preprocess'].values)), n_samples=100, isDCM=False)\n",
    "    item_tfms.append(HistScaled(bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = DataBlock(\n",
    "    blocks=(ImageBlock(cls=PILImageBW), MultiCategoryBlock),\n",
    "    get_x=ColReader('Original_Filename', pref=run_params['RAW_PREPROCESS_FOLDER']+'/', suff='.png'), \n",
    "    get_y=ColReader('Target'),\n",
    "    splitter=TestColSplitter(col='Dataset'),\n",
    "    item_tfms=item_tfms,\n",
    "    batch_tfms=batch_tfms,\n",
    ").dataloaders(label_df, bs=run_params['BATCH_SIZE'], num_workers=0)\n",
    "\n",
    "label_data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def focal_loss(input, target, reduction='mean', beta=0.5, gamma=2., eps=1e-7, **kwargs):\n",
    "    n = input.size(0)\n",
    "    iflat = torch.sigmoid(input).view(n, -1).clamp(eps, 1-eps)\n",
    "    tflat = target.view(n, -1)\n",
    "    focal = -(beta*tflat*(1-iflat).pow(gamma)*iflat.log()+\n",
    "             (1-beta)*(1-tflat)*iflat.pow(gamma)*(1-iflat).log()).mean(-1)\n",
    "    if torch.isnan(focal.mean()) or torch.isinf(focal.mean()):\n",
    "        pdb.set_trace()\n",
    "    if reduction == 'mean':\n",
    "        return focal.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return focal.sum()\n",
    "    else:\n",
    "        return focal\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, beta=0.5, gamma=2., reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input, target, **kwargs):\n",
    "        return focal_loss(input, target, beta=self.beta, gamma=self.gamma, reduction=self.reduction, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks that will be used during training\n",
    "callback_fns = [\n",
    "        MixUp(),\n",
    "        # partial(OverSamplingCallback),\n",
    "        # ShowGraph\n",
    "    ]\n",
    "roc_auc = RocAuc()\n",
    "f1_score = F1ScoreMulti(average='macro')\n",
    "precision = PrecisionMulti(average='macro')\n",
    "recall = RecallMulti(average='macro')\n",
    "learn = cnn_learner(\n",
    "    label_data,\n",
    "    run_params['MODEL'],\n",
    "    loss_func=FocalLoss(),\n",
    "    metrics=[\n",
    "        accuracy_multi,\n",
    "        # roc_auc, # Not able if in some step/epoch there is only one class\n",
    "        f1_score,\n",
    "        precision,\n",
    "        recall\n",
    "    ],\n",
    "    callback_fns=callback_fns\n",
    ")\n",
    "\n",
    "# Regularization by using float precision of 16 bits\n",
    "# This helps to not overfit because is more difficult to \"memorize\" images, but enough to learn\n",
    "learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_params['USE_SAVED_MODEL']:\n",
    "    classes = label_df['Target'].unique()\n",
    "    n_out = len(classes)\n",
    "    \n",
    "    body = create_model(run_params['MODEL'], n_out, pretrained=True, n_in=1, bn_final=True)\n",
    "\n",
    "    load_model(file=run_params['PRETRAINED_MODEL_SAVE_NAME'], model=body, opt=None, with_opt=False, device=torch.cuda.current_device(), strict=False)\n",
    "    body = body[0]\n",
    "\n",
    "    nf = num_features_model(nn.Sequential(*body.children())) * 2\n",
    "    head = create_head(nf, n_out, concat_pool=True, bn_final=True)\n",
    "        \n",
    "    model = nn.Sequential(body, head)\n",
    "    apply_init(model[1], nn.init.kaiming_normal_)\n",
    "\n",
    "    learn.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(10, 0.05, freeze_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(max_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_params['SAVE_MODEL']:\n",
    "\n",
    "    if not os.path.exists(run_params['MODELS_FOLDER']):\n",
    "        os.makedirs(run_params['MODELS_FOLDER'])\n",
    "\n",
    "    save_model(file=run_params['MODEL_SAVE_PATH'], model=learn.model, opt=learn.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the top K images with largest loss\n",
    "\n",
    "from fastai.interpret import ClassificationInterpretation\n",
    "# from fastai2_extensions.interpret.all import *\n",
    "# from fastai_amalgam.interpret.all import *\n",
    "\n",
    "k = 9\n",
    "largest = True\n",
    "dls_idx = 1\n",
    "\n",
    "preds, targs, decoded, all_losses = learn.get_preds(dls_idx, with_loss=True)\n",
    "losses, idx = all_losses.topk(ifnone(k, len(all_losses)), largest=largest)\n",
    "\n",
    "top_losses_dl = learn.dls.test_dl(learn.dls[dls_idx].items.iloc[idx])\n",
    "top_losses_dl.bs = len(idx)\n",
    "\n",
    "interp = ClassificationInterpretation(\n",
    "    learn.dls[dls_idx],\n",
    "    inputs=first(top_losses_dl),\n",
    "    preds=preds[idx],\n",
    "    targs=targs[idx],\n",
    "    decoded=decoded[idx],\n",
    "    losses=losses,\n",
    "    # *tuple(map(lambda x: x[idx], learn.get_preds(dls_idx, with_input=True, with_loss=True, with_decoded=True)))\n",
    ")\n",
    "interp.plot_top_losses(k=k, cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GradCAM for the top K images with largest loss\n",
    "\n",
    "from fastai_amalgam.interpret.gradcam import gradcam\n",
    "\n",
    "for i in idx:\n",
    "    gcam = gradcam(learn, learn.dls[dls_idx].items.iloc[i.numpy()]['Raw_preprocess'], labels=['0', '1'], show_original=True, cmap=plt.cm.bone)\n",
    "    display(gcam)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GradCAM for the true positive images\n",
    "\n",
    "from fastai_amalgam.interpret.gradcam import gradcam\n",
    "\n",
    "dls_idx = 0\n",
    "label_idxs = learn.dls[dls_idx].items[learn.dls[dls_idx].items['Target'] != '0'].index\n",
    "\n",
    "for i in label_idxs:\n",
    "    gcam = gradcam(learn, learn.dls[dls_idx].items.loc[i, 'Raw_preprocess'], labels=['0', '1'], show_original=True, cmap=plt.cm.bone)\n",
    "    display(gcam)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.11 64-bit ('radiology_ai': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73c679d2cc001810287e7be6e0757dc766f509031127652202e04048c8fced99"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}