{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.11 64-bit ('radiology_ai': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73c679d2cc001810287e7be6e0757dc766f509031127652202e04048c8fced99"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define if we are on Colab and mount drive { display-mode: \"form\" }\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (COLAB ONLY) Clone GitHub repo { display-mode: \"form\" }\n",
    "\n",
    "if IN_COLAB:\n",
    "  !git clone https://github.com/lluissalord/radiology_ai.git\n",
    "\n",
    "  %cd radiology_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup environment and Colab general variables { display-mode: \"form\" }\n",
    "# %%capture\n",
    "%run colab_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Move images from Drive to temporary folder here to be able to train models { display-mode: \"form\" }\n",
    "# %%capture\n",
    "%run move_raw_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import *\n",
    "from fastai.data.transforms import *\n",
    "from fastai.vision import models\n",
    "from fastai.vision.augment import *\n",
    "from fastai.vision.core import PILImageBW\n",
    "from fastai.vision.data import *\n",
    "from fastai.vision.learner import create_cnn_model\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semisupervised.fixmatch.losses import FixMatchLoss\n",
    "from semisupervised.fixmatch.callback import FixMatchCallback\n",
    "from semisupervised.ema import EMAModel\n",
    "\n",
    "# Required to load DICOM on the fly\n",
    "from preprocessing import PILDicom_scaled, init_bins, HistScaled\n",
    "\n",
    "from utils import seed_everything, concat_templates, create_model, TestColSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "HIST_SCALED = False\n",
    "HIST_SCALED_SELF = True\n",
    "\n",
    "LR = 0.002\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "RESIZE = 512\n",
    "RANDOM_RESIZE_CROP = 256\n",
    "\n",
    "MU = 7\n",
    "LABEL_THRESHOLD = 0.95\n",
    "\n",
    "LAMBDA_U = 1\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "MODEL = models.resnet18\n",
    "# MODEL = 'efficientnet-b0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "item_tfms = [\n",
    "    Resize(RESIZE, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n",
    "    # RandomResizedCrop(RANDOM_RESIZE_CROP),\n",
    "]\n",
    "\n",
    "label_transform = [\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "    Flip(),\n",
    "    # Normalize()\n",
    "]\n",
    "\n",
    "class Multiply_255(Transform):\n",
    "    def encodes(self, o): return o * 255\n",
    "\n",
    "weak_transform = [\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "    Flip(),\n",
    "    # Multiply_255(),\n",
    "    # Normalize()\n",
    "]\n",
    "\n",
    "strong_transform = [\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "    Flip(),\n",
    "    Rotate(90),\n",
    "    Brightness(),\n",
    "    Contrast(),\n",
    "    RandomErasing(),\n",
    "    # Multiply_255(),\n",
    "    # Normalize()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "\n",
    "cbs = None\n",
    "cbs = [\n",
    "    TensorBoardCallback(),\n",
    "    MixUp,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = concat_templates(organize_folder, excel=True)\n",
    "# df.to_excel(\n",
    "#     os.path.join(PATH_PREFIX, 'all.xlsx'),\n",
    "#     index=False\n",
    "# )\n",
    "df = pd.read_excel(os.path.join(PATH_PREFIX, 'all.xlsx'), dtype={'ID':'string','Target':'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "relation_df = pd.read_csv(os.path.join(PATH_PREFIX, 'relation.csv'))\n",
    "relation_df = relation_df.set_index('Filename')\n",
    "\n",
    "final_df = df.set_index('ID').merge(relation_df, left_index=True, right_index=True)\n",
    "final_df['ID'] = final_df.index.values\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df['Raw_preprocess'] = final_df['Original_Filename'].apply(lambda filename: os.path.join(raw_preprocess_folder, filename + '.png'))\n",
    "\n",
    "unlabel_df = final_df[df['Target'].isnull()].reset_index(drop=True)\n",
    "label_df = final_df[df['Target'].notnull()].reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "  train_df, test_df = train_test_split(label_df, test_size=TEST_SIZE, shuffle=True, stratify=label_df['Target'], random_state=SEED)\n",
    "except ValueError:\n",
    "  train_df, test_df = train_test_split(label_df, test_size=TEST_SIZE, shuffle=True, random_state=SEED)\n",
    "\n",
    "try:\n",
    "  train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE/(1-TEST_SIZE), shuffle=True, stratify=train_df['Target'], random_state=SEED)\n",
    "except ValueError:\n",
    "  train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE/(1-TEST_SIZE), shuffle=True, random_state=SEED)\n",
    "\n",
    "label_df.loc[train_df.index, 'Dataset'] = 'train'\n",
    "label_df.loc[val_df.index, 'Dataset'] = 'valid'\n",
    "label_df.loc[test_df.index, 'Dataset'] = 'test'\n",
    "\n",
    "sort_dataset = {'train': 0, 'valid': 1, 'test': 2}\n",
    "label_df = label_df.sort_values('Dataset', key=lambda x: x.map(sort_dataset)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram scaling DICOM on the fly\n",
    "\n",
    "if HIST_SCALED:\n",
    "    if HIST_SCALED_SELF:\n",
    "        bins = None\n",
    "    else:\n",
    "        # bins = init_bins(fnames=L(list(final_df['Original'].values)), n_samples=100)\n",
    "        bins = init_bins(fnames=L(list(final_df['Raw_preprocess'].values)), n_samples=100, isDCM=False)\n",
    "    # item_tfms.append(HistScaled(bins))\n",
    "    item_tfms.append(HistScaled_all(bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds_params = {\n",
    "    'get_x': ColReader('Original_Filename', pref=raw_preprocess_folder+'/', suff='.png'),\n",
    "    # 'get_x': ColReader('Original'),\n",
    "    'item_tfms': item_tfms\n",
    "}\n",
    "\n",
    "label_ds_params = base_ds_params.copy()\n",
    "label_ds_params['blocks'] = (ImageBlock(cls=PILImageBW), MultiCategoryBlock)\n",
    "# label_ds_params['blocks'] = (ImageBlock(cls=PILDicom_scaled), MultiCategoryBlock)\n",
    "label_ds_params['get_y'] = ColReader('Target')\n",
    "label_ds_params['splitter'] = TestColSplitter(col='Dataset')\n",
    "label_ds_params['batch_tfms'] = label_transform\n",
    "\n",
    "unlabel_ds_params = base_ds_params.copy()\n",
    "unlabel_ds_params['blocks'] = (ImageBlock(cls=PILImageBW))\n",
    "# unlabel_ds_params['blocks'] = (ImageBlock(cls=PILDicom_scaled))\n",
    "unlabel_ds_params['splitter'] = RandomSplitter(0)\n",
    "\n",
    "dls_params = {\n",
    "    'bs': BATCH_SIZE,\n",
    "    'num_workers': 0,\n",
    "    'shuffle_train': True,\n",
    "    'drop_last': True\n",
    "}\n",
    "\n",
    "unlabel_dls_params = dls_params.copy()\n",
    "unlabel_dls_params['bs'] = BATCH_SIZE * MU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "print(f'==> Preparing label dataloaders')\n",
    "\n",
    "label_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params)\n",
    "\n",
    "# Calculate sample weights to balance the DataLoader \n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(label_dl.items['Target'])\n",
    "class_weights = {}\n",
    "for c in count:\n",
    "  class_weights[c] = 1/count[c]\n",
    "wgts = label_dl.items['Target'].map(class_weights).values[:len(train_df)]\n",
    "\n",
    "# Create weigthed dataloader\n",
    "weighted_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params, dl_type=WeightedDL, wgts=wgts)\n",
    "label_dl.train = weighted_dl.train\n",
    "\n",
    "print(f'==> Preparing unlabel dataloaders')\n",
    "\n",
    "unlabel_dl = DataBlock(\n",
    "    **unlabel_ds_params,\n",
    ").dataloaders(unlabel_df, **unlabel_dls_params)\n",
    "\n",
    "weak_transform_dl = DataBlock(\n",
    "    **unlabel_ds_params,\n",
    "    batch_tfms = weak_transform\n",
    ").dataloaders(unlabel_df, **unlabel_dls_params)\n",
    "\n",
    "strong_transform_dl = DataBlock(\n",
    "    **unlabel_ds_params,\n",
    "    batch_tfms = strong_transform\n",
    ").dataloaders(unlabel_df, **unlabel_dls_params)\n",
    "\n",
    "print(f'==> Preparing MixMatch callback')\n",
    "\n",
    "fix_match_cb = FixMatchCallback(unlabel_dl, weak_transform_dl, strong_transform_dl)\n",
    "if cbs is None:\n",
    "    cbs = [fix_match_cb]\n",
    "else:\n",
    "    cbs.append(fix_match_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling\n",
    "sched = {'lr': SchedCos(LR, LR*math.cos(7*math.pi/16))}\n",
    "cbs.append(ParamScheduler(sched))\n",
    "moms = (MOMENTUM) # 0.9 according to FixMatch paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print(\"==> creating model\")\n",
    "\n",
    "classes = label_df['Target'].unique()\n",
    "n_out = len(classes)\n",
    "\n",
    "model = create_model(MODEL, n_out, pretrained=True, n_in=1)\n",
    "\n",
    "cbs.append(EMAModel(alpha=EMA_DECAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "print(\"==> defining loss\")\n",
    "\n",
    "# DO NOT USE CLASS WEIGHT IF WEIGHTED SAMPLER IS BEING USED\n",
    "# class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['Target'])\n",
    "# class_weight = torch.as_tensor(class_weight).float()\n",
    "# if torch.cuda.is_available():\n",
    "#     class_weight = class_weight.cuda()\n",
    "class_weight = None\n",
    "\n",
    "train_criterion = FixMatchLoss(unlabel_dl=unlabel_dl, n_out=n_out, bs=BATCH_SIZE, mu=MU, lambda_u=LAMBDA_U, label_threshold=LABEL_THRESHOLD, weight=class_weight)\n",
    "criterion = train_criterion.Lx_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner\n",
    "print(\"==> defining learner\")\n",
    "\n",
    "Lx_metric = AvgMetric(func=criterion)\n",
    "Lu_metric = AvgMetric(func=train_criterion.Lu_criterion)\n",
    "\n",
    "f1_score = F1ScoreMulti(average='macro')\n",
    "precision = PrecisionMulti(average='macro')\n",
    "recall = RecallMulti(average='macro')\n",
    "fastai_metrics = [\n",
    "    # Lx_metric, Lu_metric, \n",
    "    f1_score,\n",
    "    precision, recall\n",
    "]\n",
    "\n",
    "learn = Learner(label_dl, model, loss_func=train_criterion, opt_func=SGD, lr=LR, metrics=fastai_metrics, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5, 0.001, freeze_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.interpret import ClassificationInterpretation\n",
    "\n",
    "interp = ClassificationInterpretation.from_learner(learn, ds_idx=1)\n",
    "interp.plot_top_losses(k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}