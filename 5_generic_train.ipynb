{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.11 64-bit ('radiology_ai': conda)",
      "metadata": {
        "interpreter": {
          "hash": "73c679d2cc001810287e7be6e0757dc766f509031127652202e04048c8fced99"
        }
      }
    },
    "colab": {
      "name": "8_self_supervised.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNVBYRs20LuP"
      },
      "source": [
        "#@title Define if we are on Colab and mount drive { display-mode: \"form\" }\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTVYFJLi0LuR"
      },
      "source": [
        "#@title (COLAB ONLY) Clone GitHub repo { display-mode: \"form\" }\n",
        "\n",
        "if IN_COLAB:\n",
        "  !git clone https://github.com/lluissalord/radiology_ai.git\n",
        "\n",
        "  %cd radiology_ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI74zRR30LuR"
      },
      "source": [
        "#@title Setup environment and Colab general variables { display-mode: \"form\" }\n",
        "# %%capture\n",
        "%run colab_setup.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSiXwErZ0LuR"
      },
      "source": [
        "#@title Move images from Drive to temporary folder here to be able to train models { display-mode: \"form\" }\n",
        "# %%capture\n",
        "%run move_raw_preprocess.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hg2rdo10LuS"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from fastai.basics import *\n",
        "from fastai.callback.all import *\n",
        "from fastai.data.block import *\n",
        "from fastai.data.transforms import *\n",
        "from fastai.vision import models\n",
        "from fastai.vision.augment import *\n",
        "from fastai.vision.core import PILImageBW, PILImage\n",
        "from fastai.vision.data import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHP8hO5n0LuS"
      },
      "source": [
        "SELF_SUPERVISED = False\n",
        "\n",
        "if SELF_SUPERVISED:\n",
        "    import pl_bolts\n",
        "    from pl_bolts.models.self_supervised import SimCLR\n",
        "    from pl_bolts.models.self_supervised.simclr import SimCLRTrainDataTransform, SimCLREvalDataTransform\n",
        "    from pytorch_lightning import Trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Xy_3As0LuS"
      },
      "source": [
        "from preprocessing import init_bins, HistScaled\n",
        "\n",
        "from utils import seed_everything, concat_templates, create_model, TestColSplitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfeOxI7n0LuS"
      },
      "source": [
        "SSL_MIX_MATCH = 'MixMatch'\n",
        "SSL_FIX_MATCH = 'FixMatch'\n",
        "\n",
        "SSL = SSL_FIX_MATCH\n",
        "\n",
        "if SSL == SSL_FIX_MATCH:\n",
        "    from semisupervised.fixmatch.losses import FixMatchLoss as SSLLoss\n",
        "    from semisupervised.fixmatch.callback import FixMatchCallback as SSLCallback\n",
        "elif SSL == SSL_MIX_MATCH:\n",
        "    from semisupervised.mixmatch.losses import MixMatchLoss as SSLLoss\n",
        "    from semisupervised.mixmatch.callback import MixMatchCallback as SSLCallback\n",
        "\n",
        "from semisupervised.ema import EMAModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxB821yt0LuT"
      },
      "source": [
        "TEST_SIZE = 0.15\n",
        "VALID_SIZE = 0.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjzPt9Ea0LuT"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "HIST_SCALED = False\n",
        "HIST_SCALED_SELF = True\n",
        "\n",
        "CLASS_WEIGHT = False\n",
        "WEIGTHED_SAMPLER = False\n",
        "ALL_LABELS_IN_BATCH = True\n",
        "MIN_SAMPLES_PER_LABEL = 1\n",
        "\n",
        "LR = 0.002\n",
        "\n",
        "RESIZE = 384\n",
        "RANDOM_RESIZE_CROP = 256\n",
        "\n",
        "SELF_SUPERVISED_BATCH_SIZE = 64\n",
        "\n",
        "if SSL == SSL_FIX_MATCH:\n",
        "    BATCH_SIZE = 8\n",
        "    MOMENTUM = 0.9\n",
        "    LAMBDA_U = 1\n",
        "    MU = 5\n",
        "    LABEL_THRESHOLD = 0.95\n",
        "\n",
        "    cb_params = {}\n",
        "\n",
        "    loss_params = {\n",
        "        'bs': BATCH_SIZE,\n",
        "        'mu': MU,\n",
        "        'lambda_u': LAMBDA_U,\n",
        "        'label_threshold': LABEL_THRESHOLD\n",
        "    }\n",
        "elif SSL == SSL_MIX_MATCH:\n",
        "    BATCH_SIZE = 16\n",
        "    LAMBDA_U = 75\n",
        "    T = 0.5\n",
        "    ALPHA = 0.75\n",
        "\n",
        "    cb_params = {\n",
        "        'T': T\n",
        "    }\n",
        "\n",
        "    loss_params = {\n",
        "        'bs': BATCH_SIZE,\n",
        "        'lambda_u': LAMBDA_U,\n",
        "    }\n",
        "\n",
        "\n",
        "EMA_DECAY = 0.999\n",
        "\n",
        "MODEL = models.resnet18\n",
        "# MODEL = 'efficientnet-b0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y407niUQ0LuT"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9ulgPZD0LuU"
      },
      "source": [
        "# Transformations\n",
        "\n",
        "item_tfms = [\n",
        "    Resize(RESIZE, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n",
        "    # RandomResizedCrop(RANDOM_RESIZE_CROP),\n",
        "]\n",
        "\n",
        "label_transform = [\n",
        "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
        "    Flip(),\n",
        "    # Normalize()\n",
        "]\n",
        "\n",
        "# class Multiply_255(Transform):\n",
        "#     def encodes(self, o): return o * 255\n",
        "\n",
        "unlabel_batch_tfms = [None]\n",
        "if SSL == SSL_FIX_MATCH:\n",
        "\n",
        "    weak_transform = [\n",
        "        RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
        "        Flip(),\n",
        "        # Multiply_255(),\n",
        "        # Normalize()\n",
        "    ]\n",
        "    unlabel_batch_tfms.append(weak_transform)\n",
        "\n",
        "    strong_transform = [\n",
        "        RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
        "        Flip(),\n",
        "        Rotate(90),\n",
        "        Brightness(),\n",
        "        Contrast(),\n",
        "        RandomErasing(),\n",
        "        # Multiply_255(),\n",
        "        # Normalize()\n",
        "    ]\n",
        "    unlabel_batch_tfms.append(strong_transform)\n",
        "\n",
        "elif SSL == SSL_MIX_MATCH:\n",
        "\n",
        "    unlabel_transform = [\n",
        "        RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
        "        Flip(),\n",
        "        Rotate(180, p=1),\n",
        "        # Multiply_255(),\n",
        "        # Normalize()\n",
        "    ]\n",
        "    unlabel_batch_tfms.append(unlabel_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItBoJQEZ0LuU"
      },
      "source": [
        "# Callbacks\n",
        "from fastai.callback.tensorboard import TensorBoardCallback\n",
        "\n",
        "cbs = None\n",
        "cbs = [\n",
        "    TensorBoardCallback(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyoeBxLT0LuU"
      },
      "source": [
        "if IN_COLAB:\n",
        "  df = concat_templates(organize_folder, excel=True)\n",
        "  df.to_excel(\n",
        "      os.path.join(PATH_PREFIX, 'all.xlsx'),\n",
        "      index=False\n",
        "  )\n",
        "else:\n",
        "  df = pd.read_excel(os.path.join(PATH_PREFIX, 'all.xlsx'), dtype={'ID':'string','Target':'string'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl1x6cE20LuU"
      },
      "source": [
        "# Data\n",
        "\n",
        "# Load DataFrame of relation between Original Filename and ID (IMG_XXX)\n",
        "relation_df = pd.read_csv(os.path.join(PATH_PREFIX, 'relation.csv'))\n",
        "relation_df = relation_df.set_index('Filename')\n",
        "\n",
        "# Merge data to be able to load directly from preprocessed PNG file\n",
        "final_df = df.set_index('ID').merge(relation_df, left_index=True, right_index=True)\n",
        "final_df['ID'] = final_df.index.values\n",
        "final_df = final_df.reset_index(drop=True)\n",
        "final_df['Raw_preprocess'] = final_df['Original_Filename'].apply(lambda filename: os.path.join(raw_preprocess_folder, filename + '.png'))\n",
        "\n",
        "# Load DataFrame containing labels of OOS classifier ('ap', 'other')\n",
        "metadata_labels_path = os.path.join(PATH_PREFIX, 'metadata_labels.csv')\n",
        "metadata_labels = pd.read_csv(metadata_labels_path)\n",
        "metadata_labels = metadata_labels.set_index('Path')\n",
        "\n",
        "# Merge all the data we have with the labelling in order to split correctly according to OOS classifier\n",
        "unlabel_all_df = metadata_labels.merge(final_df.set_index('Raw_preprocess'), how='left', left_index=True, right_index=True)\n",
        "unlabel_all_df = unlabel_all_df[unlabel_all_df.Target.isnull()]\n",
        "unlabel_all_df['Raw_preprocess'] = unlabel_all_df.index.values\n",
        "\n",
        "# Define which column to use as the prediction\n",
        "if 'Final_pred' in unlabel_all_df.columns:\n",
        "    pred_col = 'Final_pred'\n",
        "else:\n",
        "    pred_col = 'Pred'\n",
        "\n",
        "# Conditions for AP radiographies on unlabel data\n",
        "ap_match = (unlabel_all_df[pred_col] == 'ap') & (unlabel_all_df.Incorrect_image.isnull())\n",
        "\n",
        "# Split between label_df (labelled data), `unlabel_df` (containing only AP) and `unlabel_other_df` (with the rest of unlabel data)\n",
        "label_df = final_df[final_df['Target'].notnull()].reset_index(drop=True)\n",
        "unlabel_df = unlabel_all_df[ap_match].reset_index(drop=True)\n",
        "unlabel_other_df = unlabel_all_df[~ap_match].reset_index(drop=True)\n",
        "\n",
        "print(f'Currently {len(label_df.index)} data have been labelled')\n",
        "print(f'Remaining {len(unlabel_df.index)} data to be labelled')\n",
        "print(f'Discarded {len(unlabel_other_df.index)} data')\n",
        "\n",
        "# Split between train, valid and test\n",
        "try:\n",
        "  train_df, test_df = train_test_split(label_df, test_size=TEST_SIZE, shuffle=True, stratify=label_df['Target'], random_state=SEED)\n",
        "except ValueError:\n",
        "  train_df, test_df = train_test_split(label_df, test_size=TEST_SIZE, shuffle=True, random_state=SEED)\n",
        "\n",
        "try:\n",
        "  train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE/(1-TEST_SIZE), shuffle=True, stratify=train_df['Target'], random_state=SEED)\n",
        "except ValueError:\n",
        "  train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE/(1-TEST_SIZE), shuffle=True, random_state=SEED)\n",
        "\n",
        "label_df.loc[train_df.index, 'Dataset'] = 'train'\n",
        "label_df.loc[val_df.index, 'Dataset'] = 'valid'\n",
        "label_df.loc[test_df.index, 'Dataset'] = 'test'\n",
        "\n",
        "print('\\nSplit of labelled data is:')\n",
        "display(label_df['Dataset'].value_counts())\n",
        "\n",
        "sort_dataset = {'train': 0, 'valid': 1, 'test': 2}\n",
        "label_df = label_df.sort_values('Dataset', key=lambda x: x.map(sort_dataset)).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkYDWIen0LuU"
      },
      "source": [
        "# Histogram scaling DICOM on the fly\n",
        "\n",
        "if HIST_SCALED:\n",
        "    if HIST_SCALED_SELF:\n",
        "        bins = None\n",
        "    else:\n",
        "        # bins = init_bins(fnames=L(list(final_df['Original'].values)), n_samples=100)\n",
        "        all_valid_raw_preprocess = pd.concat([pd.Series(unlabel_all_df.index), label_df['Raw_preprocess']])\n",
        "        bins = init_bins(fnames=L(list(all_valid_raw_preprocess.values)), n_samples=100, isDCM=False)\n",
        "    # item_tfms.append(HistScaled(bins))\n",
        "    item_tfms.append(HistScaled_all(bins))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMGNEOsw0LuW"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import PIL\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SelfSupervisedDataset(Dataset):\n",
        "    def __init__(self, df, validation = False, transform=None, src_folder=raw_preprocess_folder+'/'):\n",
        "                    \n",
        "        suffix = '.png'\n",
        "        self.transform = transform\n",
        "\n",
        "        #use sklearn's module to return training data and test data\n",
        "        if validation:\n",
        "            _, self.df = train_test_split(df, test_size=0.20, random_state=42)\n",
        "\n",
        "        else:\n",
        "            self.df, _ = train_test_split(df, test_size=0.20, random_state=42)\n",
        "\n",
        "        self.image_pairs = []\n",
        "\n",
        "        for idx, d in tqdm(enumerate(self.df['Original_Filename']), total=len(self.df.index)):\n",
        "          \n",
        "            im = PIL.Image.open(src_folder + d + suffix).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                sample = self.transform(im) #applies the SIMCLR transform required, including new rotation\n",
        "            else:\n",
        "                sample = im\n",
        "\n",
        "            self.image_pairs.append(sample)\n",
        "          \n",
        "    def __len__(self):\n",
        "        return len(self.df.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #doing the PIL.image.open and transform stuff here is quite slow\n",
        "        return (self.image_pairs[idx], 0)\n",
        "\n",
        "if SELF_SUPERVISED:\n",
        "    dataset = SelfSupervisedDataset(final_df, validation = False, transform = SimCLRTrainDataTransform(min(RESIZE, RANDOM_RESIZE_CROP)))\n",
        "    val_dataset = SelfSupervisedDataset(final_df, validation = True, transform = SimCLREvalDataTransform(min(RESIZE, RANDOM_RESIZE_CROP)))\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                              batch_size=SELF_SUPERVISED_BATCH_SIZE,\n",
        "                                              num_workers=0)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                              batch_size=SELF_SUPERVISED_BATCH_SIZE,\n",
        "                                              num_workers=0)\n",
        "    num_samples = len(dataset)\n",
        "\n",
        "    #init model with batch size, num_samples (len of data), epochs to train, and autofinds learning rate\n",
        "    model_self_sup = SimCLR(gpus = 1, max_epochs=1, arch='resnet50', dataset='', batch_size = SELF_SUPERVISED_BATCH_SIZE, num_samples = num_samples)\n",
        "\n",
        "    trainer = Trainer(gpus = 1)\n",
        "    try:\n",
        "      trainer.fit(model_self_sup, data_loader, val_loader)\n",
        "    except IndexError as e:\n",
        "      print('Finish traininig due to IndexError: ', e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wobtRme0LuV"
      },
      "source": [
        "## Define Dataset parameters\n",
        "\n",
        "base_ds_params = {\n",
        "    # 'get_x': ColReader('Original_Filename', pref=raw_preprocess_folder+'/', suff='.png'),\n",
        "    'get_x': ColReader('Raw_preprocess'),\n",
        "    # 'get_x': ColReader('Original'),\n",
        "    'item_tfms': item_tfms\n",
        "}\n",
        "\n",
        "# Specific parameters for Label Dataset \n",
        "label_ds_params = base_ds_params.copy()\n",
        "if SELF_SUPERVISED:\n",
        "    label_ds_params['blocks'] = (ImageBlock(cls=PILImage), CategoryBlock)\n",
        "else:\n",
        "    label_ds_params['blocks'] = (ImageBlock(cls=PILImageBW), CategoryBlock)\n",
        "# label_ds_params['blocks'] = (ImageBlock(cls=PILDicom_scaled), MultiCategoryBlock)\n",
        "\n",
        "label_ds_params['get_y'] = ColReader('Target')\n",
        "label_ds_params['splitter'] = TestColSplitter(col='Dataset')\n",
        "label_ds_params['batch_tfms'] = label_transform\n",
        "\n",
        "\n",
        "# Specific parameters for Unlabel Dataset \n",
        "unlabel_ds_params = base_ds_params.copy()\n",
        "if SELF_SUPERVISED:\n",
        "    unlabel_ds_params['blocks'] = (ImageBlock(cls=PILImage))\n",
        "else:\n",
        "    unlabel_ds_params['blocks'] = (ImageBlock(cls=PILImageBW))\n",
        "# unlabel_ds_params['blocks'] = (ImageBlock(cls=PILDicom_scaled))\n",
        "\n",
        "unlabel_ds_params['splitter'] = RandomSplitter(0)\n",
        "\n",
        "\n",
        "## Define DataLoaders parameters\n",
        "dls_params = {\n",
        "    'bs': BATCH_SIZE,\n",
        "    'num_workers': 0,\n",
        "    'shuffle_train': True,\n",
        "    'drop_last': True\n",
        "}\n",
        "\n",
        "unlabel_dls_params = dls_params.copy()\n",
        "if SSL == SSL_FIX_MATCH:\n",
        "    unlabel_dls_params['bs'] = BATCH_SIZE * MU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AllLabelsInBatchDL(TfmdDL):\n",
        "    \"\"\" DataLoader which allows to have a minimum of samples of all the labels in each batch \"\"\"\n",
        "    def __init__(self, dataset=None, min_samples=1, **kwargs):\n",
        "        super().__init__(dataset=dataset, **kwargs)\n",
        "        if self.bs < len(self.vocab):\n",
        "            print('AllLabelsInBatchDL working as simple DL because batch size is less than number of labels')\n",
        "            self.min_samples = 0\n",
        "        else:\n",
        "            self.min_samples = min_samples\n",
        "\n",
        "    def get_idxs(self):\n",
        "        if self.n==0: return []\n",
        "        idxs = super().get_idxs()\n",
        "        if not self.shuffle: return idxs\n",
        "\n",
        "        # Transform to numpy array to replace efficiently\n",
        "        idxs = np.array(idxs)\n",
        "\n",
        "        # Generate random indexes which will be substituted by the labels\n",
        "        n_batches = self.n // self.bs\n",
        "        idxs_subs = [np.random.choice(self.bs, len(self.vocab) * self.min_samples, replace=False) + i * self.bs for i in range(n_batches)]\n",
        "\n",
        "        # Iterate along batches and substitute selected indexes with label indexes\n",
        "        for batch_idxs_subs in idxs_subs:\n",
        "            label_idxs = []\n",
        "            for label in self.vocab:\n",
        "                # Extract indexes of current label and randomly choose `min_samples`\n",
        "                label_idx = list(self.items[self.col_reader[1](self.items) == label].index)\n",
        "                label_idx = list(np.random.choice(label_idx, size=self.min_samples, replace=True))\n",
        "\n",
        "                label_idxs = label_idxs + label_idx\n",
        "            \n",
        "            # Shuffle label indexes and replace them\n",
        "            np.random.shuffle(label_idxs)\n",
        "            idxs[batch_idxs_subs] = label_idxs\n",
        "        \n",
        "        return idxs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1nsP31A0LuV"
      },
      "source": [
        "# DataLoaders\n",
        "print(f'==> Preparing label dataloaders')\n",
        "\n",
        "label_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params)\n",
        "\n",
        "if ALL_LABELS_IN_BATCH:\n",
        "    # Create DataLoader which allows to have a minimum of samples of all the labels in each batch\n",
        "    new_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params, dl_type=AllLabelsInBatchDL, min_samples=MIN_SAMPLES_PER_LABEL)\n",
        "    label_dl.train = new_dl.train\n",
        "\n",
        "elif WEIGTHED_SAMPLER:\n",
        "    # Calculate sample weights to balance the DataLoader \n",
        "    from collections import Counter\n",
        "\n",
        "    count = Counter(label_dl.items['Target'])\n",
        "    class_weights = {}\n",
        "    for c in count:\n",
        "        class_weights[c] = 1/count[c]\n",
        "    wgts = label_dl.items['Target'].map(class_weights).values[:len(train_df)]\n",
        "\n",
        "    # Create weigthed dataloader\n",
        "    weighted_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params, dl_type=WeightedDL, wgts=wgts)\n",
        "    label_dl.train = weighted_dl.train\n",
        "\n",
        "print(f'==> Preparing unlabel dataloaders')\n",
        "\n",
        "unlabel_dls = [\n",
        "    DataBlock(\n",
        "        **unlabel_ds_params,\n",
        "        batch_tfms = batch_tfms\n",
        "    ).dataloaders(unlabel_df, **unlabel_dls_params) \n",
        "    for batch_tfms in unlabel_batch_tfms\n",
        "]\n",
        "print(f'==> Preparing SSL callback')\n",
        "\n",
        "ssl_cb = SSLCallback(*unlabel_dls, **cb_params)\n",
        "if cbs is None:\n",
        "    cbs = [ssl_cb]\n",
        "else:\n",
        "    cbs.append(ssl_cb)\n",
        "\n",
        "if SSL == SSL_MIX_MATCH:\n",
        "    cbs.append(MixUp(alpha=ALPHA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG_AJc4G0LuW"
      },
      "source": [
        "# Scheduling\n",
        "if SSL == SSL_FIX_MATCH:\n",
        "    sched = {'lr': SchedCos(LR, LR*math.cos(7*math.pi/16))}\n",
        "    cbs.append(ParamScheduler(sched))\n",
        "    moms = (MOMENTUM) # 0.9 according to FixMatch paper\n",
        "    opt_func = SGD\n",
        "else:\n",
        "    opt_func = Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQqUVs--0LuX"
      },
      "source": [
        "# Model\n",
        "# from fastai.vision.learner import create_head\n",
        "from fastai.layers import *\n",
        "\n",
        "print(\"==> creating model\")\n",
        "\n",
        "classes = label_df['Target'].unique()\n",
        "n_out = len(classes)\n",
        "\n",
        "if SELF_SUPERVISED:\n",
        "    concat_pool = True\n",
        "    for i, layer_block in enumerate(model_self_sup.children()):\n",
        "      if i == 1:\n",
        "        for layer in layer_block.children():\n",
        "          for j, layer_ in enumerate(layer.children()):\n",
        "            if j == 3:\n",
        "              nf = layer_.out_features\n",
        "    # nf = num_features_model(nn.Sequential(*model_self_sup.children())) * (2 if concat_pool else 1)\n",
        "    # head = create_head(nf, n_out, lin_ftrs=[512], ps=0.5, concat_pool=concat_pool, bn_final=True)\n",
        "\n",
        "    # Seems there is somekind of issue and nf only can be 2048\n",
        "    nf = 2048\n",
        "    layers = [\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(nf, n_out),\n",
        "        nn.BatchNorm1d(n_out, momentum=0.01)\n",
        "    ]\n",
        "    head = nn.Sequential(*layers)\n",
        "    model = nn.Sequential(model_self_sup, head)\n",
        "else:\n",
        "    model = create_model(MODEL, n_out, pretrained=True, n_in=1, bn_final=True)\n",
        "\n",
        "# Initialize last BatchNorm bias with values reflecting the current probabilities with Softmax\n",
        "with torch.no_grad():\n",
        "    for name, param in model[-1][-1].named_parameters():\n",
        "        if 'bias' in name:\n",
        "            param.copy_(torch.as_tensor([np.log(p) for p in train_df['Target'].value_counts(normalize=True).values]))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()\n",
        "\n",
        "if SSL == SSL_MIX_MATCH:\n",
        "    loss_params['model'] = model\n",
        "\n",
        "cbs.append(EMAModel(alpha=EMA_DECAY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6_7MdkV0LuY"
      },
      "source": [
        "# Loss\n",
        "print(\"==> defining loss\")\n",
        "\n",
        "if CLASS_WEIGHT:\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['Target'])\n",
        "    \n",
        "    # Correct the class weights in case of using AllLabelsInBatchDL\n",
        "    if ALL_LABELS_IN_BATCH:\n",
        "        coef = MIN_SAMPLES_PER_LABEL * train_df['Target'].nunique() / BATCH_SIZE\n",
        "        class_weights *= 1 - coef\n",
        "        class_weights += np.ones_like(class_weights) * coef\n",
        "\n",
        "    class_weights = torch.as_tensor(class_weights).float()\n",
        "    if torch.cuda.is_available():\n",
        "        class_weights = class_weights.cuda()\n",
        "else:\n",
        "    class_weights = None\n",
        "\n",
        "train_criterion = SSLLoss(unlabel_dl=unlabel_dls[0], n_out=n_out, weight=class_weights, **loss_params)\n",
        "criterion = train_criterion.Lx_criterion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jniT_h530LuY"
      },
      "source": [
        "# Learner\n",
        "print(\"==> defining learner\")\n",
        "\n",
        "Lx_metric = AvgMetric(func=criterion)\n",
        "Lu_metric = AvgMetric(func=train_criterion.Lu_criterion)\n",
        "\n",
        "# Adapt metrics depending on the number of labels\n",
        "if n_out == 2:\n",
        "    average = 'binary'\n",
        "    roc_auc = RocAucBinary()\n",
        "else:\n",
        "    average = 'macro'\n",
        "    roc_auc = RocAuc()\n",
        "\n",
        "metrics = [\n",
        "    error_rate,\n",
        "    BalancedAccuracy(),\n",
        "    # roc_auc,\n",
        "    FBeta(0.5, average=average),\n",
        "    F1Score(average=average),\n",
        "    FBeta(2, average=average),\n",
        "    Precision(average=average),\n",
        "    Recall(average=average)\n",
        "]\n",
        "\n",
        "learn = Learner(label_dl, model, loss_func=train_criterion, opt_func=opt_func, lr=LR, metrics=metrics, cbs=cbs)\n",
        "learn.recorder.train_metrics = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn.freeze()\n",
        "# learn.lr_find()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4WrDDE40LuY"
      },
      "source": [
        "# learn.unfreeze()\n",
        "# learn.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learn.fine_tune(10, 0.05, freeze_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnkNpHUg0LuY"
      },
      "source": [
        "from fastai.interpret import ClassificationInterpretation\n",
        "\n",
        "interp = ClassificationInterpretation.from_learner(learn, ds_idx=1)\n",
        "interp.plot_top_losses(k=8)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}