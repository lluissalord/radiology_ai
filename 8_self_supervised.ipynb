{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.11 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "73c679d2cc001810287e7be6e0757dc766f509031127652202e04048c8fced99"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define if we are on Colab and mount drive { display-mode: \"form\" }\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (COLAB ONLY) Clone GitHub repo { display-mode: \"form\" }\n",
    "\n",
    "if IN_COLAB:\n",
    "  !git clone https://github.com/lluissalord/radiology_ai.git\n",
    "\n",
    "  %cd radiology_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup environment and Colab general variables { display-mode: \"form\" }\n",
    "# %%capture\n",
    "%run colab_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Move images from Drive to temporary folder here to be able to train models { display-mode: \"form\" }\n",
    "# %%capture\n",
    "%run move_raw_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.data.block import *\n",
    "from fastai.data.transforms import *\n",
    "from fastai.vision import models\n",
    "from fastai.vision.augment import *\n",
    "from fastai.vision.core import PILImageBW\n",
    "from fastai.vision.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_SUPERVISED = True\n",
    "\n",
    "if SELF_SUPERVISED:\n",
    "    import pl_bolts\n",
    "    from pl_bolts.models.self_supervised import SimCLR\n",
    "    from pl_bolts.models.self_supervised.simclr import SimCLRTrainDataTransform, SimCLREvalDataTransform\n",
    "    from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to load DICOM on the fly\n",
    "from preprocessing import PILDicom_scaled, init_bins, HistScaled\n",
    "\n",
    "from utils import seed_everything, concat_templates, create_model, TestColSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSL_MIX_MATCH = 'MixMatch'\n",
    "SSL_FIX_MATCH = 'FixMatch'\n",
    "\n",
    "SSL = SSL_FIX_MATCH\n",
    "\n",
    "if SSL == SSL_FIX_MATCH:\n",
    "    from semisupervised.fixmatch.losses import FixMatchLoss as SSLLoss\n",
    "    from semisupervised.fixmatch.callback import FixMatchCallback as SSLCallback\n",
    "elif SSL == SSL_MIX_MATCH:\n",
    "    from semisupervised.mixmatch.losses import MixMatchLoss as SSLLoss\n",
    "    from semisupervised.mixmatch.callback import MixMatchCallback as SSLCallback\n",
    "\n",
    "from semisupervised.ema import EMAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "HIST_SCALED = False\n",
    "HIST_SCALED_SELF = True\n",
    "\n",
    "CLASS_WEIGHT = False\n",
    "WEIGTHED_SAMPLER = True\n",
    "\n",
    "LR = 0.002\n",
    "\n",
    "RESIZE = 128\n",
    "RANDOM_RESIZE_CROP = 256\n",
    "\n",
    "SELF_SUPERVISED_BATCH_SIZE = 8\n",
    "\n",
    "if SSL == SSL_FIX_MATCH:\n",
    "    BATCH_SIZE = 8\n",
    "    MOMENTUM = 0.9\n",
    "    LAMBDA_U = 1\n",
    "    MU = 5\n",
    "    LABEL_THRESHOLD = 0.95\n",
    "\n",
    "    cb_params = {}\n",
    "\n",
    "    loss_params = {\n",
    "        'bs': BATCH_SIZE,\n",
    "        'mu': MU,\n",
    "        'lambda_u': LAMBDA_U,\n",
    "        'label_threshold': LABEL_THRESHOLD\n",
    "    }\n",
    "elif SSL == SSL_MIX_MATCH:\n",
    "    BATCH_SIZE = 16\n",
    "    LAMBDA_U = 75\n",
    "    T = 0.5\n",
    "    ALPHA = 0.75\n",
    "\n",
    "    cb_params = {\n",
    "        'T': T\n",
    "    }\n",
    "\n",
    "    loss_params = {\n",
    "        'bs': BATCH_SIZE,\n",
    "        'lambda_u': LAMBDA_U,\n",
    "    }\n",
    "\n",
    "\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "MODEL = models.resnet18\n",
    "# MODEL = 'efficientnet-b0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "item_tfms = [\n",
    "    Resize(RESIZE, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n",
    "    # RandomResizedCrop(RANDOM_RESIZE_CROP),\n",
    "]\n",
    "\n",
    "label_transform = [\n",
    "    RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "    Flip(),\n",
    "    # Normalize()\n",
    "]\n",
    "\n",
    "# class Multiply_255(Transform):\n",
    "#     def encodes(self, o): return o * 255\n",
    "\n",
    "unlabel_batch_tfms = [None]\n",
    "if SSL == SSL_FIX_MATCH:\n",
    "\n",
    "    weak_transform = [\n",
    "        RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "        Flip(),\n",
    "        # Multiply_255(),\n",
    "        # Normalize()\n",
    "    ]\n",
    "    unlabel_batch_tfms.append(weak_transform)\n",
    "\n",
    "    strong_transform = [\n",
    "        RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "        Flip(),\n",
    "        Rotate(90),\n",
    "        Brightness(),\n",
    "        Contrast(),\n",
    "        RandomErasing(),\n",
    "        # Multiply_255(),\n",
    "        # Normalize()\n",
    "    ]\n",
    "    unlabel_batch_tfms.append(strong_transform)\n",
    "\n",
    "elif SSL == SSL_MIX_MATCH:\n",
    "\n",
    "    unlabel_transform = [\n",
    "        RandomResizedCropGPU(RANDOM_RESIZE_CROP),\n",
    "        Flip(),\n",
    "        Rotate(180, p=1),\n",
    "        # Multiply_255(),\n",
    "        # Normalize()\n",
    "    ]\n",
    "    unlabel_batch_tfms.append(unlabel_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "\n",
    "cbs = None\n",
    "cbs = [\n",
    "    TensorBoardCallback(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = concat_templates(organize_folder, excel=True)\n",
    "# df.to_excel(\n",
    "#     os.path.join(PATH_PREFIX, 'all.xlsx'),\n",
    "#     index=False\n",
    "# )\n",
    "df = pd.read_excel(os.path.join(PATH_PREFIX, 'all.xlsx'), dtype={'ID':'string','Target':'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_df = pd.read_csv(os.path.join(PATH_PREFIX, 'relation.csv'))\n",
    "relation_df = relation_df.set_index('Filename')\n",
    "\n",
    "final_df = df.set_index('ID').merge(relation_df, left_index=True, right_index=True)\n",
    "final_df['ID'] = final_df.index.values\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df['Raw_preprocess'] = final_df['Original_Filename'].apply(lambda filename: os.path.join(raw_preprocess_folder, filename + '.png'))\n",
    "\n",
    "unlabel_df = final_df[df['Target'].isnull()].reset_index(drop=True)\n",
    "label_df = final_df[df['Target'].notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "relation_df = pd.read_csv(os.path.join(PATH_PREFIX, 'relation.csv'))\n",
    "relation_df = relation_df.set_index('Filename')\n",
    "\n",
    "final_df = df.set_index('ID').merge(relation_df, left_index=True, right_index=True)\n",
    "final_df['ID'] = final_df.index.values\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df['Raw_preprocess'] = final_df['Original_Filename'].apply(lambda filename: os.path.join(raw_preprocess_folder, filename + '.png'))\n",
    "\n",
    "unlabel_df = final_df[df['Target'].isnull()].reset_index(drop=True)\n",
    "label_df = final_df[df['Target'].notnull()].reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "  train_df, test_df = train_test_split(label_df, test_size=TEST_SIZE, shuffle=True, stratify=label_df['Target'], random_state=SEED)\n",
    "except ValueError:\n",
    "  train_df, test_df = train_test_split(label_df, test_size=TEST_SIZE, shuffle=True, random_state=SEED)\n",
    "\n",
    "try:\n",
    "  train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE/(1-TEST_SIZE), shuffle=True, stratify=train_df['Target'], random_state=SEED)\n",
    "except ValueError:\n",
    "  train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE/(1-TEST_SIZE), shuffle=True, random_state=SEED)\n",
    "\n",
    "label_df.loc[train_df.index, 'Dataset'] = 'train'\n",
    "label_df.loc[val_df.index, 'Dataset'] = 'valid'\n",
    "label_df.loc[test_df.index, 'Dataset'] = 'test'\n",
    "\n",
    "sort_dataset = {'train': 0, 'valid': 1, 'test': 2}\n",
    "label_df = label_df.sort_values('Dataset', key=lambda x: x.map(sort_dataset)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram scaling DICOM on the fly\n",
    "\n",
    "if HIST_SCALED:\n",
    "    if HIST_SCALED_SELF:\n",
    "        bins = None\n",
    "    else:\n",
    "        # bins = init_bins(fnames=L(list(final_df['Original'].values)), n_samples=100)\n",
    "        bins = init_bins(fnames=L(list(final_df['Raw_preprocess'].values)), n_samples=100, isDCM=False)\n",
    "    # item_tfms.append(HistScaled(bins))\n",
    "    item_tfms.append(HistScaled_all(bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds_params = {\n",
    "    'get_x': ColReader('Original_Filename', pref=raw_preprocess_folder+'/', suff='.png'),\n",
    "    # 'get_x': ColReader('Original'),\n",
    "    'item_tfms': item_tfms\n",
    "}\n",
    "\n",
    "label_ds_params = base_ds_params.copy()\n",
    "# label_ds_params['blocks'] = (ImageBlock(cls=PILImageBW), MultiCategoryBlock if label_df['Target'].nunique() > 2 else CategoryBlock)\n",
    "label_ds_params['blocks'] = (ImageBlock(cls=PILImageBW), CategoryBlock)\n",
    "# label_ds_params['blocks'] = (ImageBlock(cls=PILDicom_scaled), MultiCategoryBlock)\n",
    "label_ds_params['get_y'] = ColReader('Target')\n",
    "label_ds_params['splitter'] = TestColSplitter(col='Dataset')\n",
    "label_ds_params['batch_tfms'] = label_transform\n",
    "\n",
    "unlabel_ds_params = base_ds_params.copy()\n",
    "unlabel_ds_params['blocks'] = (ImageBlock(cls=PILImageBW))\n",
    "# unlabel_ds_params['blocks'] = (ImageBlock(cls=PILDicom_scaled))\n",
    "unlabel_ds_params['splitter'] = RandomSplitter(0)\n",
    "\n",
    "dls_params = {\n",
    "    'bs': BATCH_SIZE,\n",
    "    'num_workers': 0,\n",
    "    'shuffle_train': True,\n",
    "    'drop_last': True\n",
    "}\n",
    "\n",
    "unlabel_dls_params = dls_params.copy()\n",
    "if SSL == SSL_FIX_MATCH:\n",
    "    unlabel_dls_params['bs'] = BATCH_SIZE * MU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "print(f'==> Preparing label dataloaders')\n",
    "\n",
    "label_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params)\n",
    "\n",
    "if WEIGTHED_SAMPLER:\n",
    "    # Calculate sample weights to balance the DataLoader \n",
    "    from collections import Counter\n",
    "\n",
    "    count = Counter(label_dl.items['Target'])\n",
    "    class_weights = {}\n",
    "    for c in count:\n",
    "        class_weights[c] = 1/count[c]\n",
    "    wgts = label_dl.items['Target'].map(class_weights).values[:len(train_df)]\n",
    "\n",
    "    # Create weigthed dataloader\n",
    "    weighted_dl = DataBlock(**label_ds_params).dataloaders(label_df, **dls_params, dl_type=WeightedDL, wgts=wgts)\n",
    "    label_dl.train = weighted_dl.train\n",
    "\n",
    "print(f'==> Preparing unlabel dataloaders')\n",
    "\n",
    "unlabel_dls = [\n",
    "    DataBlock(\n",
    "        **unlabel_ds_params,\n",
    "        batch_tfms = batch_tfms\n",
    "    ).dataloaders(unlabel_df, **unlabel_dls_params) \n",
    "    for batch_tfms in unlabel_batch_tfms\n",
    "]\n",
    "print(f'==> Preparing SSL callback')\n",
    "\n",
    "ssl_cb = SSLCallback(*unlabel_dls, **cb_params)\n",
    "if cbs is None:\n",
    "    cbs = [ssl_cb]\n",
    "else:\n",
    "    cbs.append(ssl_cb)\n",
    "\n",
    "if SSL == SSL_MIX_MATCH:\n",
    "    cbs.append(MixUp(alpha=ALPHA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, validation = False, transform=None, src_folder=raw_preprocess_folder+'/'):\n",
    "                    \n",
    "        suffix = '.png'\n",
    "        self.transform = transform\n",
    "\n",
    "        #use sklearn's module to return training data and test data\n",
    "        if validation:\n",
    "            _, self.df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "        else:\n",
    "            self.df, _ = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "        self.image_pairs = []\n",
    "\n",
    "        for idx, d in tqdm(enumerate(self.df['Original_Filename']), total=len(self.df.index)):\n",
    "          \n",
    "            im = PIL.Image.open(src_folder + d + suffix).convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(im) #applies the SIMCLR transform required, including new rotation\n",
    "            else:\n",
    "                sample = im\n",
    "\n",
    "            self.image_pairs.append(sample)\n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #doing the PIL.image.open and transform stuff here is quite slow\n",
    "        return (self.image_pairs[idx], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self supervised DataLoader from Fastai DataBlock/Dataloader\n",
    "\n",
    "# self_sup_ds_params = unlabel_ds_params.copy()\n",
    "# self_sup_ds_params['splitter'] = RandomSplitter(0.2)\n",
    "# self_sup_ds_params['batch_tfms'] = SimCLRTrainDataTransform(RESIZE)\n",
    "\n",
    "# self_sup_dls_params = dls_params.copy()\n",
    "# self_sup_dls_params['bs'] = SELF_SUPERVISED_BATCH_SIZE\n",
    "\n",
    "# # Create initial Fastai Dataloader for loading images\n",
    "# init_self_sup_dl = DataBlock(\n",
    "#     **self_sup_ds_params,\n",
    "# ).dataloaders(final_df, **self_sup_dls_params)\n",
    "\n",
    "# Pytorch Lightning Bolt Dataloaders\n",
    "# data_loader = init_self_sup_dl.train\n",
    "# val_loader = init_self_sup_dl.valid\n",
    "\n",
    "dataset = MyDataset(final_df, validation = False, transform = SimCLRTrainDataTransform(RESIZE))\n",
    "val_dataset = MyDataset(final_df, validation = True, transform = SimCLREvalDataTransform(RESIZE))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=SELF_SUPERVISED_BATCH_SIZE,\n",
    "                                          num_workers=0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                          batch_size=SELF_SUPERVISED_BATCH_SIZE,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "\n",
    "#init model with batch size, num_samples (len of data), epochs to train, and autofinds learning rate\n",
    "model = SimCLR(gpus = 1, arch='resnet50', dataset='',batch_size = SELF_SUPERVISED_BATCH_SIZE, num_samples = num_samples)\n",
    "\n",
    "trainer = Trainer(gpus = 1)\n",
    "trainer.fit(model, data_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling\n",
    "if SSL == SSL_FIX_MATCH:\n",
    "    sched = {'lr': SchedCos(LR, LR*math.cos(7*math.pi/16))}\n",
    "    cbs.append(ParamScheduler(sched))\n",
    "    moms = (MOMENTUM) # 0.9 according to FixMatch paper\n",
    "    opt_func = SGD\n",
    "else:\n",
    "    opt_func = Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "print(\"==> creating model\")\n",
    "\n",
    "classes = label_df['Target'].unique()\n",
    "n_out = len(classes)\n",
    "\n",
    "model = create_model(MODEL, n_out, pretrained=True, n_in=1, bn_final=True)\n",
    "\n",
    "# Initialize last BatchNorm bias with values reflecting the current probabilities with Softmax\n",
    "with torch.no_grad():\n",
    "    for name, param in model[-1][-1].named_parameters():\n",
    "        if 'bias' in name:\n",
    "            param.copy_(torch.as_tensor([np.log(p) for p in train_df['Target'].value_counts(normalize=True).values]))\n",
    "\n",
    "if SSL == SSL_MIX_MATCH:\n",
    "    loss_params['model'] = model\n",
    "\n",
    "cbs.append(EMAModel(alpha=EMA_DECAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "print(\"==> defining loss\")\n",
    "\n",
    "if CLASS_WEIGHT:\n",
    "    class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['Target'])\n",
    "    class_weight = torch.as_tensor(class_weight).float()\n",
    "    if torch.cuda.is_available():\n",
    "        class_weight = class_weight.cuda()\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "train_criterion = SSLLoss(unlabel_dl=unlabel_dls[0], n_out=n_out, weight=class_weight, **loss_params)\n",
    "criterion = train_criterion.Lx_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner\n",
    "print(\"==> defining learner\")\n",
    "\n",
    "Lx_metric = AvgMetric(func=criterion)\n",
    "Lu_metric = AvgMetric(func=train_criterion.Lu_criterion)\n",
    "\n",
    "# Adapt metrics depending on the number of labels\n",
    "if n_out == 2:\n",
    "    average = 'binary'\n",
    "    roc_auc = RocAucBinary()\n",
    "else:\n",
    "    average = 'macro'\n",
    "    roc_auc = RocAuc()\n",
    "\n",
    "metrics = [\n",
    "    error_rate,\n",
    "    BalancedAccuracy(),\n",
    "    # roc_auc,\n",
    "    FBeta(0.5, average=average),\n",
    "    F1Score(average=average),\n",
    "    FBeta(2, average=average),\n",
    "    Precision(average=average),\n",
    "    Recall(average=average)\n",
    "]\n",
    "\n",
    "learn = Learner(label_dl, model, loss_func=train_criterion, opt_func=opt_func, lr=LR, metrics=metrics, cbs=cbs)\n",
    "learn.recorder.train_metrics = True"
   ]
  },
  {
   "source": [
    "learn.freeze()\n",
    "learn.lr_find()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn.fine_tune(10, 0.01, freeze_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,preds,targs,decoded,losses = learn.get_preds(dl=learn.dls[1], with_input=True, with_loss=True, with_decoded=True, act=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.interpret import ClassificationInterpretation\n",
    "\n",
    "interp = ClassificationInterpretation.from_learner(learn, ds_idx=1)\n",
    "interp.plot_top_losses(k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=8\n",
    "largest=True\n",
    "losses,idx = interp.top_losses(k=8, largest=True)\n",
    "if not isinstance(interp.inputs, tuple): interp.inputs = (interp.inputs,)\n",
    "if isinstance(interp.inputs[0], Tensor): inps = tuple(o[idx] for o in interp.inputs)\n",
    "else: inps = interp.dl.create_batch(interp.dl.before_batch([tuple(o[i] for o in interp.inputs) for i in idx]))\n",
    "b = inps + tuple(o[idx] for o in (interp.targs if is_listy(interp.targs) else (interp.targs,)))\n",
    "x,y,its = interp.dl._pre_show_batch(b, max_n=k)\n",
    "# b_out = inps + tuple(o[idx] for o in (interp.decoded if is_listy(interp.decoded) else (interp.decoded,)))\n",
    "b_out = b\n",
    "x1,y1,outs = interp.dl._pre_show_batch(b_out, max_n=k)\n",
    "if its is not None:\n",
    "    plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), interp.preds[idx], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (interp.decoded if is_listy(interp.decoded) else (interp.decoded,))\n",
    "for o in a:\n",
    "    print(o[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_out"
   ]
  }
 ]
}